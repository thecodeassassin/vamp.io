[{"path":"/about","date":"2016-09-13T09:00:00+00:00","title":"about us","content":"Vamp is being developed by Magnetic.io from the heart of beautiful Amsterdam.  \n\nWe’re dedicated to helping organisations and companies of all sizes increase efficiency, save time and reduce costs.\nWe provide powerful and easy-to-use solutions to optimise modern architectures and systems, often based around microservices and/or containers.\n\nVision\nTime is precious. We believe people should focus their efforts on the things they excel at and that matter to them. Let computers do the repeatable and automatable tasks - they’re far more suited to that than we are. Smart automation FTW!\n\n Community\nWe encourage anyone to join the Vamp community and pitch in with pull requests, bug reports and feature requests to make Vamp even more awesome.\n\nWorking at Vamp\nWe're alway on the lookout for smart, talented and motivated people that love to work on products and understand real-world problems and how to solve them in the best possible way. If you would love to join our team and work on Vamp and related products, please get in touch!\n\n Contact\nYou can contact us here\n","id":0},{"path":"/contact","date":"2016-09-13T09:00:00+00:00","title":"contact","content":"\nVamp is being developed by Magnetic.io in the heart of Amsterdam.\n\nOur Amsterdam offices:\n\nSint Antoniesbreestraat 16  \n1011 HB Amsterdam  \nThe Netherlands  \n+31(0)88 555 33 99  \ninfo@magnetic.io\n\nProfessional services and consultancy\nWe can provide professional services and consultancy around the implementation and use of Vamp. Send us an email on info@magnetic.io or call +31(0)88 555 33 99.\n\n Vamp Enterprise Edition (EE)\nWe also provide a commercial Enterprise Edition of Vamp with features specifically tuned to enterprise usage. Contact us to get more information.\n\nSupport\nCheck our support page for SLA's and other forms of support.\n\n Community\nJoin the Vamp community to make Vamp even better.\n","id":1},{"path":"/documentation/api/api-reference","date":"2016-09-13T09:00:00+00:00","title":"API Reference","content":"\nThis page gives full details of all available API calls. See using the Vamp API for details on pagination, json and yaml content types and effective use of the API.\n\nBlueprints\n\n List blueprints\n\nLists all blueprints without any pagination or filtering.\n\n    GET /api/v1/blueprints\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nGet a single blueprint\n\nLists all details for one specific blueprint.\n\n    GET /api/v1/blueprints/{blueprint_name}\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\n Create blueprint\n\nCreates a new blueprint. Accepts JSON or YAML formatted blueprints. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/blueprints\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a blueprint\n\nUpdates the content of a specific blueprint.\n\n    PUT /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 200 OK if the blueprint is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a blueprint\n\nDeletes a blueprint.        \n\n    DELETE /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the blueprint.\n\n---------,\nBreeds\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List breeds\n\nLists all breeds without any pagination or filtering.\n\n    GET /api/v1/breeds\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\nGet a single breed\n\nLists all details for one specific breed.\n\n    GET /api/v1/breeds/{breed_name}\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\n Create breed\n\nCreates a new breed. Accepts JSON or YAML formatted breeds. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/breeds\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a breed\n\nUpdates the content of a specific breed.\n\n    PUT /api/v1/breeds/{breed_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a breed\n\nDeletes a breed.        \n\n    DELETE /api/v1/breeds/{breed_name}\n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the breed.\n\n------,\nConditions\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List conditions\n\nLists all conditions without any pagination or filtering.\n\n    GET /api/v1/conditions\n\nGet a single condition\n\nLists all details for one specific condition.\n\n    GET /api/v1/conditions/{condition_name}\n\n Create condition\n\nCreates a new condition. Accepts JSON or YAML formatted conditions. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/conditions\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a condition\n\nUpdates the content of a specific condition.\n\n    PUT /api/v1/conditions/{condition_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a condition\n\nDeletes a condition.\n\n    DELETE /api/v1/conditions/{condition_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n----------,\nDeployments\n\nDeployments are non-static entities in the Vamp eco-system. They represent runtime structures so any changes to them will take time to execute and can possibly fail. Most API calls to the /deployments endpoint will therefore return a 202: Accepted return code, indicating the asynchronous nature of the call.\n\nDeployments have a set of sub resources: SLA's, scales and gateways. These are instantiations of their static counterparts.\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List deployments\n\n\tGET /api/v1/deployments\n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint      | true or false     | false            | exports each deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences  | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\nGet a single deployment\n\nLists all details for one specific deployment.\n\n    GET /api/v1/deployments/{deployment_name}\n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint     | true or false     | false            | exports the deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences   | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\n Create deployment using a blueprint\n\nCreates a new deployment\n\n\tPOST /api/v1/deployments\n\nCreate a named (non UUID) deployment\n\n\tPUT /api/v1/deployments/{deployment_name}\n\t\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the blueprint is valid for deployment. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a deployment using a blueprint\n\nUpdates the settings of a specific deployment.\n\n    PUT /api/v1/deployments/{deployment_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the update would be still valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a deployment using a blueprint\n\nDeletes all or parts of a deployment.        \n\n    DELETE /api/v1/deployments/{deployment_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the (partial) deletion would be still valid. Actual delete is not performed.\n\nIn contrast to most API's, doing a DELETE in Vamp takes a request body that designates what part of the deployment should be deleted. This allows you to remove specific services, clusters of the whole deployment.\n\n{{ note title=\"Note!\" }}\nDELETE on deployment with an empty request body will not delete anything.\n{{ /note }}\n\nThe most common way to specify what you want to delete is by exporting the target deployment as a blueprint using the ?as_blueprint=true parameter. You then either programmatically or by hand edit the resulting blueprint and specify which of the services you want to delete. You can also use the blueprint as a whole in the DELETE request. The result is the removal of the full deployment. \n\nexample - delete service\n\nThis is our (abbreviated) deployment in YAML format. We have two clusters. The first cluster 'frontend' has two services.\nWe have left out some keys like scale among others as they have no effect on this specific use case.\n\n\t\tGET /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcd?as_blueprint=true\n\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nendpoints:\n  frontend.port: '9050'\nclusters:\n  frontend:\n    services:\n    breed:\n        name: monarch_front:0.1\n        deployable: magneticio/monarch:0.1\n        ports:\n          port: 8080/http\n        constants: {}\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n    breed:\n        name: monarch_front:0.2\n        deployable: magneticio/monarch:0.2\n        ports:\n          port: 8080/http\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n  backend:\n    services:\n    breed:\n        name: monarch_backend:0.3\n        deployable: magneticio/monarch:0.3\n        ports:\n          jdbc: 8080/http\n        environment_variables: {}\n\nIf we want to delete the first service in the frontend cluster, we use the following blueprint as the request body in the DELETE action.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n\nIf we want to delete the whole deployment, we just specify all the clusters and services.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t  \n\t\t  \nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n    breed:\n        ref: monarch_front:0.2\n  backend:\n    services:\n    breed:\n        ref: monarch_backend:0.3\n\n Deployment SLAs\n\nGet a deployment SLA\n\nLists all details for a specific SLA that's part of a specific cluster.\n\n\tGET /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\t\n Set a deployment SLA\n\nCreates or updates a specific deployment SLA.\n\n\tPOST|PUT /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\t\nDelete a deployment SLA\n\nDeletes as specific deployment SLA.\n\n\tDELETE /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\n Deployment scales\n\nDeployment scales are singular resources: you only have one scale per service. Deleting a scale is not a meaningfull action.\n\nGet a deployment scale\n\nLists all details for a specific deployment scale that's part of a service inside a cluster.\n\n\tGET /api/v1/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}/scale\n\t\n Set a deployment scale\t\n\nUpdates a deployment scale.\n\n\tPOST|PUT /api/v1/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}/scale\n\n-------,\n\nEscalations\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List escalations\n\nLists all escalations without any pagination or filtering.\n\n    GET /api/v1/escalations\n\nGet a single escalation\n\nLists all details for one specific escalation.\n\n    GET /api/v1/escalations/{escalation_name}\n\n Create escalation\n\nCreates a new escalation. Accepts JSON or YAML formatted escalations. Set the Content-Type request header to application/json or application/x-yaml accordingly.   \n\n    POST /api/v1/escalations\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate an escalation\n\nUpdates the content of a specific escalation.\n\n    PUT /api/v1/escalations/{escalation_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete an escalation\n\nDeletes an escalation.        \n\n    DELETE /api/v1/escalations/{escalation_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n------,\nEvents\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List events\n\nLists metrics and/or events without any pagination or filtering.\n\n    GET /api/v1/events/get\n\n| parameter     | description      |\n| ------------- |:----------------:|\n| tag           | Event tag, e.g. GET /api/v1/events?tag=archiving&tag=breeds\n\n{{ note title=\"Note!\" }}\nsearch criteria can be set in request body, checkout examples for event stream.\n{{ /note }}\n\nCreate events\n\n    POST /api/v1/events    \n    \n Server-sent events (SSE)\n\n    GET  /api/v1/events/stream\n\n-----------,\nGateways\n\n List gateways\n\n    GET /api/v1/gateways\n\nGet a single gateway\n\n    GET /api/v1/gateways/{gateway_name}\n\n Create gateway\nAccepts JSON or YAML formatted gateways. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/gateways\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 201 Created if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a gateway\n\n    PUT /api/v1/gateways/{gateway_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 200 OK if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a gateway     \n\n    DELETE /api/v1/gateways/{gateway_name}\n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the gateway.\n\n--------,\nMetrics\n\nMetrics can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/metrics/gateways/{gateway}/{metrics}\n/api/v1/metrics/gateways/{gateway}/routes/$route/{metrics}\n\n/api/v1/metrics/deployments/{deployment}/clusters/{cluster}/ports/{port}/{metrics}\n/api/v1/metrics/deployments/{deployment}/clusters/{cluster}/services/{service}/ports/{port}/{metrics}\n\n Example\n    /api/v1/metrics/deployments/sava/clusters/frontend/ports/api/response-time\n\n{{ note title=\"Note!\" }}\nMetrics are calculated using external services, e.g. Vamp workflows.\n{{ /note }}\n\nHealth\n\nHealth can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/health/gateways/{gateway}\n/api/v1/health/gateways/{gateway}/routes/$route\n\n/api/v1/health/deployments/{deployment}\n/api/v1/health/deployments/{deployment}/clusters/{cluster}\n/api/v1/health/deployments/{deployment}/clusters/{cluster}/services/{service}\n\nHealth is value between 1 (100% healthy) and 0.\n\n{{ note title=\"Note!\" }}\nHealth is calculated using external services, e.g. Vamp workflows.\n{{ /note }}\n\n--------,\n Scales\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\nList scales\n\nLists all scales without any pagination or filtering.\n\n    GET /api/v1/scales\n\n Get a single scale\n\nLists all details for one specific scale.\n\n    GET /api/v1/scales/{scale_name}\n\nCreate scale\n\nCreates a new scale. Accepts JSON or YAML formatted scales. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/scales\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 201 Created if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Update a scale\n\nUpdates the content of a specific scale.\n\n    PUT /api/v1/scales/{scale_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 200 OK if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nDelete a scale\n\nDeletes a scale.        \n\n    DELETE /api/v1/scales/{scale_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the scale.\n\n----------,\n SLAs\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\nList SLAs\n\nLists all slas without any pagination or filtering.\n\n    GET /api/v1/slas\n\n Get a single SLA\n\nLists all details for one specific breed.\n\n    GET /api/v1/slas/{sla_name}\n\nCreate an SLA\n\nCreates a new SLA\n\n    POST /api/v1/slas   \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 201 Created if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Update an SLA\n\nUpdates the content of a specific SLA.\n\n    PUT /api/v1/slas/{sla_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 200 OK if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nDelete an SLA\n\nDeletes an SLA.        \n\n    DELETE /api/v1/slas/{sla_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the SLA.\n\n------------,\n System\n\nVamp provides a set of API endpoints that help with getting general health/configuration status.\n\nGet runtime info\n\nLists information about Vamp's JVM environment and runtime status. \nAlso lists info for configured persistence layer and container driver status.\n\n\tGET /api/v1/info\n\t\nSections are jvm, persistence, keyvalue, pulse, gatewaydriver, containerdriver and workflowdriver:\n\n{\n    \"message\": \"...\",\n    \"version\": \"...\",\n    \"uuid\": \"...\",\n    \"running_since\": \"...\",\n    \"jvm\": {...},\n    \"persistence\": {...},\n    \"key_value\": {...},\n    \"pulse\": {...},\n    \"gateway_driver\": {...},\n    \"container_driver\": {...},\n    \"workflow_driver\": {...}\n}\n\n Example - explicitly request specific sections\nExplicitly requesting jvm and persistence using parameter(s) on:\n\n\tGET /api/v1/info?on=jvm&on=persistence\n\nGet Vamp configuration\n\n\tGET /api/v1/config\n\n Get HAProxy configuration\n\n\tGET /api/v1/haproxy\n\nDebug \n\n Force sync\n\nForces Vamp to perform a synchronization cycle, regardless of the configured default interval.\n\n\tGET /api/v1/sync\n\t\nForce SLA check\t\n\nForces Vamp to perform an SLA check, regardless of the configured default interval.\n\n\tGET /api/v1/sla\n\n Force escalation\t\n\nForces Vamp to perform an escalation check, regardless of the configured default interval.\n\n\tGET /api/v1/escalation\n\n------------,See using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n","id":2},{"path":"/documentation/api/using-the-api","date":"2016-10-19T09:00:00+00:00","title":"Using the Vamp API","content":"{{ note title=\"New in Vamp 0.9.1\" }}\nWe've now added support for Websockets to our HTTP API. More information will follow soon.\n{{ /note }}\n\nVamp has one REST API. This page explains how to specify pagination, and json and yaml content types, and how to effectively use the Vamp REST API.\n\nSee also\nFull details of all available API calls\n\n Content types\n\nVamp requests can be in YAML format or JSON format. Set the Content-Type request header to application/x-yaml or application/json accordingly.\nVamp responses can be in YAML format or JSON format. Set the Accept request header to application/x-yaml or application/json accordingly.\n\nPagination\n\nVamp API endpoints support pagination with the following scheme:\n\nRequest parameters page (starting from 1, not 0) and per_page (by default 30) e.g:\n\nGET http://vamp:8080/api/v1/breeds?page=5&per_page=20\n\nResponse headers X-Total-Count giving the total amount of items (e.g. 349673) and a Link header for easy traversing, e.g.\nX-Total-Count: 5522\nLink:\n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=first,\n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=prev,\n  http://vamp:8080/api/v1/events/get?page=2&per_page=5; rel=next,\n  http://vamp:8080/api/v1/events/get?page=19&per_page=5; rel=last\n\nSee Github's implementation for more info.\n\n Return codes\n\nCreate & Delete operations are idempotent: sending the second request with the same content will not result to an error response (4xx).\nAn update will fail (4xx) if a resource does not exist.\nA successful create operation has status code 201 Created and the response body contains the created resource.\nA successful update operation has status code 200 OK or 202 Accepted and the response body contains the updated resource.\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\nSending multiple artifacts (documents) - POST, PUT and DELETE\n\nIt is possible to send YAML document containing more than 1 artifact definition:\n\nGET /api/v1\n\nSupported methods are POST, PUT and DELETE. Example:\n\n,name: ...\nkind: breed\n breed definition ...\n,name: ...\nkind: blueprint\nblueprint definition ...\n\nAdditional kind field is required and it always correspond (singular form) to type of the artifact.\nFor instance if specific endpoint would be /api/v1/deloyments then the same deployment request can be sent to api/v1 with additional kind: deployment.\nIf specific endpoints are used (e.g. /api/v1/blueprints) then kind needs to be ommited.\n\n----------,\n See also\nFull details of all available API calls\n","id":3},{"path":"/documentation/cli/cli-reference","date":"2016-09-13T09:00:00+00:00","title":"CLI reference","content":"\nThe VAMP CLI supports the following commands:  \ncreate, deploy, generate, help, info, inspect, list, merge, remove, undeploy, update, version  \nSee using the Vamp CLI for details on installation, configuration and effective use of the CLI\n\nFor details about a specific command, use vamp COMMAND --help\n\n----------,## Create\n\nCreate an artifact read from the specified filename or read from stdin.\n\nvamp create blueprint|breed|deployment|escalation|condition|scale|sla [--file|--stdin]\n\nParameter | purpose\n----------|-----,--file        |       Name of the yaml file [Optional]\n--stdin        |      Read file from stdin [Optional]\n  \nExample\n vamp create scale --file my_scale.yaml\nname: my_scale\ncpu: 2.0\nmemory: 2GB\ninstances: 2\n\n----------, Deploy\n\nDeploys a blueprint\n\nvamp deploy NAME --deployment [--file|--stdin]\n\nParameter | purpose\n----------|-----,--file      |         Name of the yaml file [Optional]\n--stdin     |         Read file from stdin [Optional]\n--deployment|         Name of the deployment to update [Optional]\n\nExample\n vamp deploy --deployment 1111-2222-3333-4444 --file mynewblueprint.yaml\n\n----------, Generate\n\nGenerates an artifact\n\nvamp generate breed|blueprint|condition|scale [NAME] [--file|--stdin]\n| Parameter | purpose |\n|-----------|---------|\n--file    |           Name of the yaml file to preload the generation [Optional]\n--stdin   |           Read file from stdin [Optional]\n\ngenerate breed\n\n| Parameter | purpose |\n|-----------|---------|\n--deployable  |       Deployable specification [Optional]\n\n Example\n vamp generate breed mynewbreed --json\n{\n  \"name\":\"mynewbreed\",\n  \"deployable\":\"docker://\",\n  \"ports\":{\n    \n  },\n  \"environment_variables\":{\n    \n  },\n  \"constants\":{\n    \n  },\n  \"dependencies\":{\n    \n  }\n}\n\ngenerate blueprint\n\n| Parameter | purpose |\n|-----------|---------|\n--cluster   |         Name of the cluster\n--breed     |         Name of the breed   [Optional, requires --cluster]\n--scale     |         Name of the scale   [Optional, requires --breed]\n\n----------, Help\n\nDisplays the Vamp help message\n\nExample\n vamp help\nUsage: vamp COMMAND [args..]\n\nCommands:\n  create              Create an artifact\n  deploy              Deploys a blueprint\n  help                This message\n  generate            Generates an artifact\n  info                Information from Vamp\n  inspect             Shows the details of the specified artifact\n  list                Shows a list of artifacts\n  merge               Merge a blueprint with an existing deployment or blueprint\n  remove              Removes an artifact\n  undeploy            Removes (part of) a deployment\n  update              Update an artifact\n  version             Shows the version of the VAMP CLI client\n  \nRun vamp COMMMAND --help  for additional help about the different command options\n\n----------, Info\n\nDisplays the Vamp Info message\n\nExample\n vamp info\nmessage: Hi, I'm Vamp! How are you?\njvm:\n  operating_system:\n    name: Mac OS X\n    architecture: x86_64\n    version: 10.9.5\n    available_processors: 8.0\n    systemloadaverage: 4.8095703125\n  runtime:\n    process: 12871@MacMatthijs-4.local\n    virtualmachinename: Java HotSpot(TM) 64-Bit Server VM\n    virtualmachinevendor: Oracle Corporation\n    virtualmachineversion: 25.31-b07\n    start_time: 1433415167162\n    up_time: 1305115\n...    \n\n----------, Inspect\nShows the details of the specified artifact\n\nvamp inspect blueprint|breed|deployment|escalation|condition|scale|sla NAME --json\n\n| Parameter | purpose |\n|-----------|---------|\n--as_blueprint | Returns a blueprint (only for inspect deployment) [Optional]|\n--json    |  Output Json instead of Yaml [Optional]|\n\nExample\n vamp inspect breed sava:1.0.0\nname: sava:1.0.0\ndeployable: magneticio/sava:1.0.0\nports:\n  port: 80/http\nenvironment_variables: {}\nconstants: {}\ndependencies: {}\n\n----------, List\nShows a list of artifacts\n\nvamp list blueprints|breeds|deployments|escalations|conditions|gateways|scales|slas\n\nExample \n vamp list deployments\nNAME                                    CLUSTERS\n80b310eb-027e-44e8-b170-5bf004119ef4    sava\n06e4ace5-41ce-46d7-b32d-01ee2c48f436    sava\na1e2a68b-295f-4c9b-bec5-64158d84cd00    sava, backend1, backend2\n\n----------, Merge\n\nMerges a blueprint with an existing deployment or blueprint.\nEither specify a deployment or blueprint in which the blueprint should be merged\nThe blueprint can be specified by NAME, read from the specified filename or read from stdin.\n\nvamp merge --deployment|--blueprint [NAME] [--file|--stdin] \n      \n| Parameter | purpose |\n|-----------|---------|\n--file     | Name of the yaml file [Optional]\n--stdin    | Read file from stdin [Optional]\n\nExample\nvamp merge --blueprint myexistingblueprint -- file addthisblueprint.yaml\n\n----------, Remove\n\nRemoves artifact\n\nvamp remove blueprint|breed|escalation|condition|scale|sla NAME\n\nExample\n vamp remove scale my_scale\n----------, Undeploy\n\nRemoves (part of) a deployment.\nBy only specifying the name, the whole deployment will be removed. To remove part of a deployment, specify a blueprint. The contents of the blueprint will be subtracted from the active deployment.\n\nvamp undeploy NAME [--blueprint|--file|--stdin] \n\nParameter | purpose\n----------|-----,--blueprint|    Name of the stored blueprint to subtract from the deployment\n--file   |       Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\nExample\n vamp undeploy 9ec50a2a-33d7-4dd3-a027-9eeaeaf925c1 --blueprint sava:1.0\n----------, Update\n\nUpdates an existing artifact read from the specified filename or read from stdin.\n\nvamp update blueprint|breed|deployment|escalation|condition|scale|sla NAME [--file] [--stdin]\n\nParameter | purpose\n----------|-----,--file   |      Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\n----------,## Version\n\nDisplays the Vamp CLI version information \n\nExample\n vamp version\nCLI version: 0.7.9\n----------,\n See also\n\nUsing the Vamp CLI - installation, configuration and effective use of the CLI","id":4},{"path":"/documentation/cli/using-the-cli","date":"2016-09-13T09:00:00+00:00","title":"Using the Vamp CLI","content":"\nVamp's command line interface (CLI) can be used to perform basic actions against the Vamp API. The CLI was\nprimarily developed to work in continuous delivery situations. In these setups, the CLI takes care of automating (canary) releasing new artifacts to Vamp deployments and clusters.\n\nSee also\nFull list of available CLI commands\n\n Installation\n\nCheck run vamp for details on how to install the Vamp CLI on your platform. \n\nConfiguration\n\nAfter installation, set Vamp's host location. This location can be specified as a command line option (--host)\n\nvamp list breeds --host=http://192.168.59.103:8080\n\n...or via the environment variable VAMP_HOST\nexport VAMP_HOST=http://192.168.59.103:8080\n\n Simple commands\n\nThe basic commands of the CLI, like list, allow you to do exactly what you would expect:\n\n vamp list breeds\nNAME                     DEPLOYABLE\ncatalog                  docker://zutherb/catalog-frontend\ncheckout                 docker://zutherb/monolithic-shop\nproduct                  docker://zutherb/product-service\nnavigation               docker://magneticio/navigation-service:latest\ncart                     docker://zutherb/cart-service\nredis                    docker://redis:latest\nmongodb                  docker://mongo:latest\nmonarch_front:0.1        docker://magneticio/monarch:0.1\nmonarch_front:0.2        docker://magneticio/monarch:0.2\nmonarch_backend:0.3      docker://magneticio/monarch:0.3\n\n vamp list deployments\nNAME                                    CLUSTERS\n1272c91b-ba29-4ad1-8d09-33cbaa8f6ac2    frontend, backend\n\nCI and chaining\n\nIn more complex continuous integration situations you can use the CLI with the --stdin flag to chain a bunch of commands together. You could for instance:\n\nget an \"old\" version of a breed with inspect\ngenerate a new breed based on the previous one, while inserting a new deployable\ncreate the breed in the backend\n\nvamp inspect breed frontend:${OLD} | \\\nvamp generate breed --deployable mycompany/frontend:${NEW} frontend:${NEW} --stdin | \\\nvamp create breed --stdin\n\nOnce you have the new breed stored, you can insert it into a running deployment at the right position, i.e:\n\nget a blueprint from a running deployment with inspect and --as_blueprint\ngenerate a new blueprint with generate while inserting a new breed\ndeploying the result with deploy\n\nvamp inspect deployment $DEPLOYMENT --as_blueprint | \\\nvamp generate blueprint --cluster frontend --breed frontend:${NEW} --stdin | \\\nvamp deploy --deployment $DEPLOYMENT --stdin\n\n------, See also\nFull list of available CLI commands","id":5},{"path":"/documentation/how vamp works/architecture-and-components","date":"2016-09-13T09:00:00+00:00","title":"Architecture and components","content":"\nArchitecture\nVamp and the Vamp Gateway Agent require specific elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. There is no set architecture required for running Vamp and every use case or specific combination of tools and platforms can have its own set up.\n\n Example topology\nThe below diagram should be used more as an overview than required architecture. For example, in this diagram the Mesos/Marathon stack and Elasticsearch are included even though these are not a hard dependency. Vamp can be configured to run with other container schedulers, log-aggregators, key-value and event-stores.\n\nVamp components\n\nVamp consists of server- and client-side components that work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation.\n\n Vamp UI  \nThe Vamp UI is a graphical web interface for managing Vamp in a web browser. It is packaged with Vamp.\n\nVamp CLI  \nThe Vamp CLI is a command line interface for managing Vamp and providing integration with (shell) scripts. It's currently not very well maintained but still can be useful if our REST API cannot be used for your integration requirements.\n\n Vamp  \nVamp is the main API endpoint, business logic and service coordinator. Vamp talks to the configured container manager (Docker, Marathon, Kubernetes etc.) and synchronizes it with Vamp Gateway Agent (VGA)  via ZooKeeper, etcd or Consul (distributed key-value stores). Vamp can use Elasticsearch for artifact persistence and to store events (e.g. changes in deployments). Typically, there should be one Vamp instance and one or more VGA instances. Vamp is not a realtime application and only updates deployments and routing when asked to (reactive) and thus doesn't need to run with multiple instances in HA mode. If this is a hard requirement of your project please contact us for the Vamp Enterprise Edition.\n\nVamp workflows\nVamp workflows are small applications or scripts (for example using JavaScript or your own containers) that automate changes of the running system, and its deployments and gateways. We have included a set of useful workflows out of the box, such as health and metrics, which are used by the Vamp UI to report system status and to enable autoscaling and self-healing. Our Vamp Runner project provides more advanced workflow recipes as an example.\n\n Vamp Gateway Agent (VGA)  \nVamp Gateway Agent (VGA) reads the HAProxy configuration from ZooKeeper, etcd or Consul and reloads HAProxy on each configuration change with as close to zero client request interruptions as possible. Typically, there should be one Vamp instance and one or more VGA instances.     \nLogs from HAProxy are read over socket and pushed to Logstash over UDP.  VGA will handle and recover from ZooKeeper, etcd, Consul and Logstash outages without interrupting the HAProxy process and client requests.  \n\n{{ note title=\"What next?\" }}\nRead about the requirements to run Vamp\n{{ /note }}\n","id":6},{"path":"/documentation/how vamp works/events-and-metrics","date":"2016-10-21T09:00:00+00:00","title":"Events and metrics","content":"\nTo provide an effective feedback loop, HTTP/TCP logs should be collected, stored and analyzed. Collection and storing is done with a combination of HAProxy, VGA and Logstash setup. Logs are stored in Elasticsearch and can be later visualised by Kibana.\n\nHow traffic is logged\n\nVamp uses Logstash to format and store logs from running applications HAProxy (via Vamp Gateway Agent) in Elasticsearch indexes. Logstash listens on UDP port and formats incoming log raw data in json format. Vamp API actions (including those generated by Vamp workflows and the Vamp UI) and running service events are stored by the Vamp API to specific Elasticsearch indexes. \nData and events are read by Vamp components either directly from Elasticsearch or via the Vamp API:\n\nThe Vamp UI reads data and events via the Vamp API. Health and Metrics events (generated by Vamp workflows) are required by the Vamp UI.\nVamp workflows read formatted log data directly from Elasticsearch and events are via the Vamp API. In theory, workflows could also read events directly from Elasticsearch.\n\n Formatted raw data (logs)\n\nHAProxy\nHAProxy generates logs and makes them accessible via open socket - check the HAProxy configuration of log (github.com/magneticio - haproxy.cfg).\nVGA listens on log socket and any new messages are forwarded to the Logstash instance. The HAProxy log format is configurable in Vamp configuration vamp.gateway-driver.haproxy (github.com/magneticio - reference.conf).\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level). \n\n Logstash\n\nA simple Logstash configuration should be sufficient for dozens of requests per second - or even more, depending on whether Elastic Stack (ELK) is also used for custom application/service logs etc. This example Logstash configuration (github.com/magneticio - logstash.conf), together with the default vamp.gateway-driver.haproxy log format, will transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualisation).  \nFor alternative Logstash/Elasticsearch setups you can check these examples (elastic.co - Deploying and Scaling Logstash) and the Logstash command line parameter (github.com/magneticio - Logstash section).\n\n{{ note title=\"Note!\" }}\nLogstash listens on UDP port, but in principle any other listener can receive logs forwarded by VGA.\nDifferent VGAs can use different Logstash instances.\n{{ /note }}\n\nEvents\n\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\nAll events are stored and retrieved using the Event API, part of the Vamp API.\n\n Kibana\n\n  Vamp can be configured to create Kibana searches, visualisations and dashboards automatically with the vamp.gateway-driver.kibana.enabled configuration parameter.\n  Vamp will do this by inserting ES documents to the Kibana index, so only the URL to access ES is needed (by default reusing the same as for persistence). Read more about Vamp configuration\n\n{{ note title=\"What next?\" }}\nLet's install Vamp \n{{ /note }}","id":7},{"path":"/documentation/how vamp works/persistence-key-value-store","date":"2016-09-13T09:00:00+00:00","title":"Persistence and key-value (KV) store","content":"\nPersistence \nVamp uses Elasticsearch (ES) as main persistence (e.g. for artifacts and events). \nVamp is not demanding in ES resources, so a small ES installation is sufficient for Vamp indexes (index names are configurable). Vamp can also use an existing ES cluster.\n\n Key-value (KV) store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances - all communication is done by managing specific KV in the store.  Currently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\n{{ note title=\"What next?\" }}\nRead about routing and load balancing\n{{ /note }}\n\n","id":8},{"path":"/documentation/how vamp works/requirements","date":"2016-09-13T09:00:00+00:00","title":"Requirements","content":"\nVamp's components work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. To achieve this, Vamp requires access to a container scheduler, key value store, Elastic Search and HAProxy.\n\nContainer scheduler  (orchestration)\nVamp talks directly to your choice of container scheduler. Currently we support Mesos/Marathon, DC/OS, Kubernetes and Rancher. In case you’re “greenfield” and don’t have anything selected or running yet, check which container scheduler?\n\n Key value store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances, all communication is done by managing specific KV in the store.  When Vamp needs to update the HAProxy configuration (e.g. when a new service has been deployed) Vamp will generate the new configuration and store it in the KV store. The VGAs read specific valuea and reload HAProxy instances accordingly.\nCurrently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\nElastic Search (persistence and metrics)\nVamp uses Elastic Search (ES) for persistence (e.g. for artifacts and events) and for aggregating the metrics used by Vamp workflows and the Vamp UI. As Vamp is not demanding in ES resources, it can comfortably work with an existing ES cluster.  \nCurrently we use Logstash to format and send data to Elastic Search, but you could also opt for an alternative solution.\n\n HAProxy  (routing)\nEach Vamp Gateway Agent (VGA) requires its own instance of HAProxy. This is a hard requirement, so to keep things simple we provide a Docker container with both Vamp Gateway Agent (VGA) and HAProxy (hub.docker.com - magneticio/vamp-gateway-agent).  \n\n{{ note title=\"What next?\" }}\nFind out how to install Vamp\nHigh level pointers for choosing a container scheduler\n{{ /note }}\n\n","id":9},{"path":"/documentation/how vamp works/routing-and-load-balancing","date":"2016-09-13T09:00:00+00:00","title":"Routing and load balancing","content":"Vamp uses the tried and tested HAProxy reverse proxy software for routing/proxying and load balancing (haproxy.com). Vamp Gateway Agent (VGA) manages the HAProxy configuration and HAProxy routes incoming traffic to endpoints (explicitly defined external gateways) or handles intra-service routing. By applying some iptables magic, Vamp makes sure that HAProxy configuration updates won't introduce dropped packages., that means zero-downtime reloads.  \n\nRouting\n\nSo how does Vamp exactly route traffic to the designated destinations? First we look for the conditions that might have been set for a route or gateway. This can be none, one or more conditions (see boolean expression in conditions). There are built-in short codes for common conditions, or you can use HAProxy ACLs directly.\n\nIf the condition is met, we evaluate the condition strength percentage. A 100% setting means everybody that meets the condition is sent to this route. A 5% setting means 5% of all visitors that meet the condition are sent to this route, the remaining 95% are returned into the \"bucket\" and distributed using the general weight settings. A weight setting for each available route defines the distribution of all remaining traffic not matching a condition or not targetted by condition strength.\n\n Load balancing\n\nVamp load balancing is done transparently. Based on the scale setting of the running services, Vamp will make sure all instances are load balanced automatically. By default we use a round-robin algorithm, but other HAProxy balancing mechanisms are also supported. We also support sticky routing. The cool thing is that weight and condition percentage settings are applied independently from the number of instances running. I.e. a 50 / 50 weight distribution over two service versions that run with a scale of four and eight instances respectively will still be distributed 50% / 50%. Changing the number of instances will have no effect on the distribution, as Vamp tries to achieve the configured weight and condition strength distributions as closely as possible.\n\nTopology and performance\n\nHAProxy can run as a container or as a standalone service. A Vamp Gateway Agent (VGA) Docker image (including HAProxy with specific logstash configuration to provide proxy logs to logstash for the Vamp UI) can be pulled from the Docker hub (hub.docker.com - magneticio Vamp Gateway Agent).\n\nHAProxy can run inside your cluster or on separate machines outside of your container cluster:\n\nHAProxy on one node of a cluster - not advised for production setups as it introduces a single point of failure\nHAProxy on multiple nodes of a cluster - for example, three instances for failover and high-availability \nHAProxy on all nodes of a cluster - the so-called SmartStack pattern (nerds.airbnb.com - smartstack service discovery cloud) \nHAProxy on separate machines outside of your container cluster - Vamp can connect to these instances and can add its routing rules to your custom HAProxy configuration templates if needed.\n\nPerformance-wise, HAProxy is very efficient and uses few resources. In our experiments we have seen a sub-millisecond overhead. Even with very complex and combined routing rules, the total overhead stays in the microseconds range. This means a single HAProxy running on a small VM (for example, an AWS micro instance) would process enough network traffic that you would notice a bottleneck from your network and applications first.\n\nVamp is not a realtime system. As long as at least one HAProxy and one container-node are running your visitors will be able to reach the container - even with no Vamp or VGA running. On restart, Vamp and VGA will automatically sync and update themselves. \n\n{{ note title=\"What next?\" }}\nRead about how Vamp works with events and metrics\nFind out more about using Vamp conditions and gateways\n{{ /note }}\n","id":10},{"path":"/documentation/how vamp works/service-discovery","date":"2016-09-13T09:00:00+00:00","title":"Service discovery","content":"\nVamp uses a service discovery pattern called server-side service discovery, which allows for service discovery without the need to change your code or run any other daemon or agent (microservices.io - server side discovery). In addition to service discovery, Vamp also functions as a service registry (microservices.io - service registry).\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\nCreate and publish a service\n\n{{ note title=\"Note\" }}\nServices do not register themselves. They are explicitly created, registered in the Vamp database and provisioned on the load balancer.\n{{ /note }}\n\nServices are created and published as follows:\n\nThe user describes a service and its desired endpoint port in the Vamp DSL.\nThe service is deployed to the configured container manager by Vamp.\nVamp instructs Vamp Gateway Agent (via ZooKeeper, etcd or Consul) to set up service endpoints.\nVamp Gateway Agent takes care of configuring HAProxy, making the services available.\n\nAfter this, you can scale the service up/down or in/out either by hand or using Vamp’s auto scaling functionality. The endpoint is stable.\n\n Discovering a service\n\nSo, how does one service find a dependent service? Services are found by just referencing them in the DSL. Take a look at the following example:\n,name: my_blueprint:1.0\nclusters:\n  myfrontendcluster:\n    services:\n      breed:\n        name: myfrontendservice:0.1\n        deployable: company/frontend:0.1\n        ports:\n          port: 8080/http\n        dependencies:\n          backend: mybackendservice:0.3\n        environment_variables:\n         BACKEND_HOST: $backend.host\n         BACKEND_PORT: $backend.ports.jdbc\n      scale:\n        instances: 3         \n  mybackendcluster:\n    services:\n      breed:\n        name: mybackendservice:0.3\n        deployable: company/backend:0.3\n        ports:\n          jdbc: 8080/tcp\n      scale:\n        instances: 4\n\nWe have a frontend cluster and a backend cluster. These are just organisational units.\nThe frontend cluster runs just one version of our service, consisting of three instances.\nThe frontend service has a hard dependency on a backend (tcp) service.\nWe reference the backend by name, my_backend:0.3, and assign it a label, in this case just backend\nWe use the label backend to get the host and a specific port (jdbc) from this backend.\nWe assign these values to environment variables that are exposed in the container runtime.\nAny frontend service now has access to the location of the dependent backend service.\n\n{{ note title=\"Note\" }}\nThere is no point-to-point wiring. The $backend.host and $backend.ports.jdbc variables resolve to service endpoints Vamp automatically sets up and exposes.\n{{ /note }}\n\nEven though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc.\n\n{{ note title=\"What next?\" }}\nRead about how Vamp works with routing and load balancing\n{{ /note }}","id":11},{"path":"/documentation/how vamp works/which-container-scheduler","date":"2016-09-13T09:00:00+00:00","title":"Which container scheduler?","content":"Vamp can run on top of Mesos/Marathon, DC/OS, Kubernetes and Rancher (Docker Swarm support is coming soon). In case you’re “greenfield” and don’t have anything selected or running yet, here are some high-level pointers to help you make an informed decision: \n\nWorking with big data - Mesos/Marathon, DC/OS, Azure Container Service\nRunning web-based applications - Kubernetes, Google Container Engine\nManaging (virtual) infrastructure - Rancher with Docker or Kubernetes\nJust running Docker\n\nWhichever option you choose now, Vamp is container systems agnostic, so all your blueprints and workflows will keep on working if you decide to switch in the future *.\n\n----,\nWorking with big data \nIf you are working with typical big data solutions like Kafka, Cassandra or Spark (often combined in something called SMACK stack), and/or want to run not only containers on your cluster it makes sense to investigate Mesos/Marathon first. A lot of big data frameworks can run as native Mesos frameworks and you can combine the underlying infrastructure to share resources between these frameworks running on Mesos and your containers running inside Marathon (which is a Mesos framework in itself).\n\n Add in commercial support\nIf you have the same requirements as described above, but you're more comfortable buying commercial support, moving towards DC/OS makes sense. DC/OS is based on Mesos/Marathon, but adds additional features and a fancy UI. You can purchase commercial DC/OS support or buy an enterprise version from Mesosphere (the company that initiated Mesos and Marathon).\n\nHosted solution\nIf you're looking for a hosted version of DC/OS you could investigate Azure Container Service which let's you choose between DC/OS or Docker Swarm.\n\n----,\n Running web-based applications  \nIf you're solely interested in running (micro)services, APIs and other web-based applications, Kubernetes is an integrated cluster-manager and -scheduler, and is specifically designed for running containers with web-focused payloads. \n\nAdd in commercial support\nAt this point, commercial support and fancy dashboards are less easy to find for Kubernetes. However, Kubernetes is the scheduler used in Redhat software (Openshift V3), so if your company used Redhat software this might make sense to investigate.\n\n hosted solution\nA hosted version of Kubernetes is available from Google (Google Container Engine). \n\n----,\nManaging (virtual) infrastructure\nIf you want to manage and provision (virtual) infrastructure as well as manage and run containers, Rancher is a viable option. Rancher provides a Docker or Kubernetes based container scheduler and adds infrastructure provisioning with a nice graphical UI.\n\n----,\n Just running Docker\nIf you want to stay within the Docker ecosystem, Vamp works nicely with (single machine) Docker. Docker Swarm support is coming up soon.\n\n----,  \n* Note that Vamp dialects and some specific metric store settings are scheduler or container cloud specific.\n\n{{ note title=\"What next?\" }}\nFind out how to install Vamp \nRead about the requirments to run Vamp \n{{ /note }}","id":12},{"path":"/documentation/installation/azure-container-service","date":"2016-09-30T12:00:00+00:00","title":"Azure Container Service","content":"\nTo run Vamp together with Azure Container Service (azure.microsoft.com - Container Service), you need to use DC/OS as the default ACS Docker container scheduler.\n\nAfter you have activated your ACS setup with DC/OS, go to your DC/OS admin environment and install Vamp using our DC/OS installation instructions.\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}\n","id":13},{"path":"/documentation/installation/configure-vamp","date":"2016-09-13T09:00:00+00:00","title":"Configure Vamp","content":"\nVamp can be configured using one or a combination of the Vamp application.conf HOCON file (github.com/typesafehub - config), environment variables and system properties.\n\nFor example:\n\nexport VAMPINFOMESSAGE=Hello # overriding Vamp info message (vamp.info.message)\n\njava -Dvamp.gateway-driver.host=localhost \\\n     -Dlogback.configurationFile=logback.xml \\\n     -Dconfig.file=application.conf \\\n     -jar vamp.jar\n\nThe Vamp application.conf file\n\nThe Vamp application.conf consists of the following sections. All sections are nested inside a parent vamp {} tag.\n\nhttp-api\npersistence\ncontainer-drivers\ngateway-driver\noperation\n\n http-api\nConfigure the port, host name and interface that Vamp runs on using the http-api.port\n\nvamp {\n  http-api {\n    interface = 0.0.0.0\n    host = localhost\n    port = 8080\n    response-timeout = 10 seconds # HTTP response time out\n  }\n}    \n\npersistence\n\n{{ note title=\"Updated for Vamp 0.9.1\" }}\nIn the Vamp configuration we set persistence caching by default to false. In our Vamp images we set this to true to make it easier on the persistence store load. Check out this default Vamp 0.9.1 configuration for reference.\nWe've added a key-value store as a persistence data store. Check out this default Vamp 0.9.1 configuration for reference.\n{{ /note }}\n\nVamp uses Elasticsearch for persistence and ZooKeeper (apache.org - ZooKeeper), etcd (coreos.com  - etcd documentation) or Consul (consul.io) for key-value store (keeping HAProxy configuration).\n\nvamp {\n  persistence {\n    response-timeout = 5 seconds\n\n    database {\n      type: \"elasticsearch\"  elasticsearch or in-memory (no persistence)\n      elasticsearch.url = ${vamp.pulse.elasticsearch.url}\n    }\n\n    key-value-store {\n\n      type = \"zookeeper\"    # zookeeper, etcd or consul\n      base-path = \"/vamp\"   # base path for keys, e.g. /vamp/...\n\n      zookeeper {\n        servers = \"192.168.99.100:2181\"\n      }\n\n      etcd {\n        url = \"http://192.168.99.100:2379\"\n      }\n\n      consul {\n        url = \"http://192.168.99.100:8500\"\n      }\n    }\n  }\n}\n\nzookeeper, etcd or consul configuration is needed based on the type, e.g. if type = \"zookeeper\" then only zookeeper.servers should be set.\n\nContainer drivers\n\nVamp can be configured to work with the following container drivers:\n\nDocker\nMesos/Marathon\nKubernetes\nRancher\n\n Docker\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing, Docker Swarm support is coming soon.\nVamp can even run inside Docker while deploying to Docker.\n\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf:\n\n        ...\n    container-driver {\n      type = \"docker\"\n      response-timeout = 30 # seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nMesos/Marathon\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver {\n      type = \"marathon\"\n      url = \"http://marathonhost:marathonport\"\n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nKubernetes\nSpecify Kubernetes as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Kubernetes (github.com/magneticio - vamp-kubernetes):\n\n  ...\n  container-driver {\n\n    type = \"kubernetes\"\n\n    kubernetes {\n      url = \"https://kubernetes\"\n      service-type = \"LoadBalancer\"\n    }\n    ...\n\n Rancher\n\nSpecify Rancher as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Rancher (github.com/magneticio - vamp-rancher):\n\n  ...\n  container-driver.type = \"rancher\"\n  ...\n\ngateway-driver\n\nThe gateway-driver section configures how traffic should be routed through Vamp Gateway Agent. See the below example on how to configure this:\n\nvamp {\n  gateway-driver {\n    host: \"10.193.238.26\"               Vamp Gateway Agent / Haproxy, internal IP.\n    response-timeout: 30 seconds\n\n    haproxy {\n      ip: 127.0.0.1                    # HAProxy backend server IP\n\n      template: \"\"                     # Path to template file, if not specified default will be used\n\n      virtual-hosts {\n        ip: \"127.0.0.1\"                # IP, if virtual hosts are enabled\n        port: 40800                    # Port, if virtual hosts are enabled\n      }\n    }\n  }\n}  \n\nThe reason for the need to configure vamp.gateway-driver.host is that when services are deployed, they need to be able to find Vamp Gateway Agent in their respective networks. This can be a totally different network than where Vamp is running.\nLet's use an example: frontend and backend service, frontend depends on backend - in Vamp DSL that would be 2 clusters (assuming the same deployment).\nThere are different ways how frontend can discover its dependency backend, and to make things simpler Vamp supports using specific environment parameters.\n\n,name: my-web-app\nclusters:\n  frontend:\n    services:\n      breed:\n        name: my-frontend:1.0.0\n        deployable: magneticio/my-frontend:1.0.0\n        ports:\n          port: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.port\n        dependencies:\n          backend: my-backend:1.0.0\n  backend:\n    services:\n      breed:\n        name: my-backend:1.0.0\n        deployable: magneticio/my-backend:1.0.0\n        ports:\n          port: 8080/http\n\nIn this example $backend.host will have the value of the vamp.gateway-driver.host configuration parameter, while $backend.ports.port the next available port from vamp.operation.gateway.port-range.\nfrontend doesn't connect to backend directly but via Vamp Gateway Agent(s) - given on these host and port parameters.\nThis is quite simmilar to common pattern to access any clustered application.\nFor instance if you want to access DB server, you will have an address string based on e.g. DNS name or something simmilar.\nNote that even without Vamp, you would need to setup access to backend in some similar way.\nWith Vamp, access is via VGA's and that allows specific routing (conditions, weights) needed for A/B testing and canary releasing.\nAdditional information can be found on service discovery page.\n\noperation\n\nThe operation section holds all parameters that control how Vamp executes against \"external\" services: this also includes Vamp Pulse and Vamp Gateway Agent.\n\noperation {\n  sla.period = 5 seconds         controls how often an SLA checks against metrics\n  escalation.period = 5 seconds # controls how often Vamp checks for escalation events\n\tsynchronization {\n    period = 4 seconds          # controls how often Vamp performs\n                                # a sync between Vamp and the container driver.\n    timeout {\n      ready-for-deployment: 600\tseconds   # controls how long Vamp waits for a\n                                          # service to start. If the service is not started\n                                          # before this time, the service is registered as \"error\"\n      ready-for-undeployment: 600 seconds # similar to \"ready-for-deployment\", but for\n                                          # the removal of services.\n    }\n   }\n\n  gateway {\n    port-range = 40000-45000\n    response-timeout = 5 seconds # timeout for container operations\n\n    virtual-hosts.formats {      # name format\n      gateway                 = \"$gateway.vamp\"\n      deployment-port         = \"$port.$deployment.vamp\"\n      deployment-cluster-port = \"$port.$cluster.$deployment.vamp\"\n    }\n  }\n\n  deployment {\n    scale {         # default scale, if not specified in blueprint\n      instances: 1\n      cpu: 1\n      memory: 1GB\n    }\n\n    arguments: []   # split by first '=',\n                    # Docker command line arguments, e.g. \"security-opt=seccomp:unconfined\"\n  }\n}\n\nFor each cluster and service port within the same cluster a gateway is created - this is exactly as one that can be created using Gateway API.\nThat means specific conditions and weights can be applied on traffic to/from cluster services - A/B testing and canary releases support.\nvamp.operation.gateway.port-range is range of port values that can be used for these cluster/port gateways.\nThese ports need to be available on all Vamp Gateway Agent hosts.\n\nEnvironment variables\n\nEach configuration parameter can be replaced by an environment variable. Environment variables have precedence over configuration from application.conf or system properties.  Read more about environment variables.\n\n Environment variable names\nEnvironment variable names are based on the configuration parameter name converted to upper case. All non-alphanumerics should be replaced by an underscore _\n\nvamp.info.message           ⇒ VAMPINFOMESSAGE\nvamp.gateway-driver.timeout ⇒ VAMPGATEWAYDRIVER_TIMEOUT\n\n{{ note title=\"What next?\" }}\nFollow the getting started tutorials\nYou can read in depth about using Vamp or browse the API reference or CLI reference docs.\n{{ /note }}\n","id":14},{"path":"/documentation/installation/dcos","date":"2016-09-30T12:00:00+00:00","title":"DC/OS 1.7 and 1.8","content":"\nThere are different ways to install Vamp on DC.OS. On this page we start out with the most common setup, but if you are interested in doing a custom install or working with public and private nodes you should jump to that section.\n\nStandard install\nCustom install\nPublic and private nodes\n\nStandard install\nThis setup will run Vamp, Mesos and Marathon, together with Zookeeper, Elasticsearch and Logstash on DC/OS. If you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n Tested against\nThis guide has been tested on both 1.7 and the latest 1.8 version of DC/OS.\n\nRequirements\nBefore you start you need to have a DC/OS cluster up and running, as well as the its CLI configured to use it. We assume you have it up and running on http://dcos.example.com/.\nSetting up DC/OS is outside the scope of this document, for that you need to refer to the official documentation:\n\nhttps://dcos.io/docs/1.7/administration/installing/\nhttps://dcos.io/docs/1.7/usage/cli/\nhttps://dcos.io/docs/1.8/administration/installing/\nhttps://dcos.io/docs/1.8/usage/cli/\n\n Step 1: Install Elasticsearch + Logstash\n\nMesos, Marathon and ZooKeeper are all installed by DC/OS. In addition to these, Vamp requires Elasticsearch and Logstash for metrics collection and aggregation.\n\nYou could install Elasticsearch on DC/OS by following the Mesos Elasticsearch documentation (mesos-elasticsearch - Elasticsearch Mesos Framework).\nHowever, Vamp will also need Logstash (not currently available as a DC/OS package) with a specific Vamp Logstash configuration (github.com/magneticio - Vamp Docker logstash.conf).  \n\nTo make life easier, we have created compatible Docker images for a Vamp Elastic Stack (hub.docker.com - magneticio elastic) that you can use with the Mesos elasticsearch documentation (mesos-elasticsearch - How to install on Marathon).\nOur advice is to use our custom Elasticsearch+Logstash Docker image. Let's get started!\n\nCreate elasticsearch.json with the following content:\n\n{\n  \"id\": \"elasticsearch\",\n  \"instances\": 1,\n  \"cpus\": 0.2,\n  \"mem\": 1024.0,\n  \"container\": {\n    \"docker\": {\n      \"image\": \"magneticio/elastic:2.2\",\n      \"network\": \"HOST\",\n      \"forcePullImage\": true\n    }\n  },\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"port\": 9200,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis will run the container with 1G of RAM and a basic health check on the elasticsearch port.\n\nUsing the CLI we can install this in our cluster:\n\n$ dcos marathon app add elasticsearch.json\n\nIf you get no error message you should now be able to see it being deployed:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    0/1    0/0      scale       DOCKER   None  \n\nOnce it's fully up and running you should see all tasks and health checks being up:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n\nStep 2: Deploy Vamp\n\nOnce you have elasticsearch up and running it's time to move on to Vamp. The Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nCreate vamp.json with the following content:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://10.20.0.100:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },  \n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis service definition will download our Vamp container and spin it up in your DC/OS cluster on a private node in bridge networking mode. It will also configure the apporiate labels for the AdminRouter to expose the UI through DC/OS, as well as an internal VIP for other applications to talk to Vamp, adjusting some defaults to work inside DC/OS, and finally a health check for monitoring.\n\nDeploy it with the CLI, like with did with elasticsearch:\n\n$ dcos marathon app add vamp.json\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n/vamp/vamp      1024  0.5    0/1    0/0      scale       DOCKER   None  \n\nIt will take a minute for Vamp to deploy all its components, you can see that by looking in the \"tasks\" column, where Vamp is listed as 0/1. Run the list command again and you should see all the components coming online:\n\n$ dcos marathon app list\nID                        MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD\n/elasticsearch            1024  0.2    1/1    1/1       ---        DOCKER   None\n/vamp/vamp                1024  0.5    1/1    1/1       ---        DOCKER   None\n/vamp/vamp-gateway-agent  256   0.2    3/3    ---       ---        DOCKER   ['--storeType=zookeeper', '--storeConnection=zk-1.zk:2181', '--storeKey=/vamp/gateways/haproxy/1.6', '--logstash=elasticsearch.marathon.mesos:10001']\n/vamp/workflow-health      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-kibana      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-metrics     64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-vga         64   0.1    1/1    ---       ---        DOCKER   None\n\nVamp has now spun up all it's components and you should be able to access the ui by opening http://dcos.example.com/service/vamp/ in your browser.\n\nNow you're ready to follow our Vamp getting started tutorials.\nThings still not running? We're here to help →\n\n NB If you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\n Custom install\n\nThe Vamp DC/OS Docker image (github.com/magneticio - Vamp DC/OS) contains configuration (github.com/magneticio - Vamp DC/OS configuration) that can be overridden for specific needs by:\n\nMaking a new Docker image based on the Vamp DC/OS image\nUsing environment variables\n\nExample 1 - Remove the metrics and health workflows by Vamp configuration and keep the kibana workflow:\n\nvamp.lifter.artifact.resources = [\n    \"breeds/kibana.js\", \"workflows/kibana.yml\"\n  ]\n\nor doing the same using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_RESOURCES\": \"[\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\"\n}\n\n Example 2 - Avoid automatic deployment of Vamp Gateway Agent\n\nRemove vga-marathon breed and workflow from vamp.lifter.artifact.files:\n\nvamp.lifter.artifact.files = []\n\nor using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_FILES\": \"[]\"\n}\n\nPublic and private nodes\n\nRunning Vamp on public Mesos agent node(s) and disabling automatic Vamp Gateway Agent deployments (but keeping other default workflows) can be done with the following Marathon JSON:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPLIFTERARTIFACT_FILES\": \"[\\\"breeds/health.js\\\",\\\"workflows/health.yml\\\",\\\"breeds/metrics.js\\\",\\\"workflows/metrics.yml\\\",\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\",\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://vamp-vamp.marathon.mesos:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },\n  \"acceptedResourceRoles\": [\n    \"slave_public\"\n  ],\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nDeploying Vamp Gateway Agent on all public and private Mesos agent nodes through Marathon JSON - NB replace $INSTANCES (e.g. to be the same as total number of Mesos agent nodes) and optionally other parameters:\n\n{\n  \"id\": \"vamp/vamp-gateway-agent\",\n  \"instances\": $INSTANCES,\n  \"cpus\": 0.2,\n  \"mem\": 256.0,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp-gateway-agent:katana\",\n      \"network\": \"HOST\",\n      \"privileged\": true,\n      \"forcePullImage\": true\n    }\n  },\n  \"args\": [\n    \"--storeType=zookeeper\",\n    \"--storeConnection=zk-1.zk:2181\",\n    \"--storeKey=/vamp/gateways/haproxy/1.6\",\n    \"--logstash=elasticsearch.marathon.mesos:10001\"\n  ],\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ],\n  \"acceptedResourceRoles\": [\n    \"slave_public\",\n    \"*\"\n  ]\n}\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can follow our getting started tutorials.\nChcek the Vamp documentation\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}","id":15},{"path":"/documentation/installation/docker","date":"2016-09-30T12:00:00+00:00","title":"Docker","content":"\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing. Vamp can even run inside Docker while deploying to Docker.  You can pass native Docker options by using the Docker dialect in a Vamp blueprint.. Docker Swarm support is coming soon.\n\nSet container driver\nSee set Docker as the container driver\n\n","id":16},{"path":"/documentation/installation/hello-world","date":"2016-09-13T09:00:00+00:00","title":"Hello world","content":"\nThe Vamp hello world setup will run Mesos, Marathon (mesosphere.github.io - Marathon) and Vamp 0.9.1 inside a local Docker container with Vamp's Marathon driver.  We will do this in three simple steps (although it's really just one docker run command). You can use the hello world setup to work through the getting started tutorials and try out some of Vamp's core features.\n\n{{ note }}\nThis hello world set up is designed for demo purposes only - it is not production grade.\n{{ /note }}\n\nRequirements\nAt least 8GB free space\n\n Get Docker\n\nPlease install one of the following for your platform/architecture\n\nDocker 1.9.x (Linux) or higher (Vamp works with Docker 1.12 too), OR\n[Docker Toolbox 1.12.x] (https://github.com/docker/toolbox/releases) if on Mac OS X 10.8+ or Windows 7+ \n\nVamp hello world on Docker for Mac or Windows is currently not supported. We're working on this so please check back. \n\nRun Vamp\n\nStart the magneticio/vamp-docker:0.9.1-marathon container, taking care to pass in the right parameters for your system: \n\n Mac OS X 10.8+ or Windows 7+\n\nIf you installed Docker Toolbox, please use the Docker Quickstart Terminal. We don't currently support Kitematic. A typical command on Mac OS X running Docker Toolbox would be:\ndocker run --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v docker-machine ssh default \"which docker\":/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=docker-machine ip default\" \\\n           magneticio/vamp-docker:0.9.1\n\nLinux\n\ndocker run --privileged \\\n           --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v $(which docker):/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=hostname -I | awk '{print $1;}'\" \\\n           magneticio/vamp-docker:0.9.1\n\nMounting volumes is important. Read this great article about starting Docker containers from/within another Docker container.\n\n Check Vamp is up and running\n\nAfter some downloading and booting, your Docker log will show the Vamp has launched and report:  \n...Binding: 0.0.0.0:8080\n\nNow you can check if Vamp is home on http://{docker-machine ip default}:8080/ and you're ready for the Vamp getting started tutorials\n\n  \nAll the services exposed in this demo are listed below. If you run on Docker machine you will need to switch localhost for docker-machine ip default.\n\nExposed services |  \n----------|-----,HAProxy statistics        |       http://localhost:1988 (username/password: haproxy)\nElasticsearch HTTP        |      http://localhost:9200\nKibana        |       http://localhost:5601\nSense        |      http://localhost:5601/app/sense\nMesos        |       http://localhost:5050\nMarathon       |      http://localhost:9090 (Note that the Marathon port is 9090 and not the default 8080)\nChronos        |       http://localhost:4400\nVamp UI       |      http://localhost:8080\n\nSumming up\n\nThis set up runs all of Vamp's components in one container. You will run into cpu, memory and storage issues pretty soon though. Also, random ports are assigned by Vamp which you might not have exposed on either Docker or your Docker Toolbox Vagrant box.  This is definitely not ideal, but works fine for kicking the tires.\nNow you're all set to follow our getting started tutorials.\n\n{{ note title=\"What next?\" }}\nFollow the getting started tutorials.\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}","id":17},{"path":"/documentation/installation/","date":"2016-09-13T09:00:00+00:00","title":"Installation","content":"Before you get Vamp up and running on your architecture, it is helpful to understand how vamp works and the role of each component and its preferred location in a typical architecture.  \n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nRequirements\n\nVamp requirements\n\n Install Vamp\nThe Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nDC/OS 1.7 and 1.8\nMesos/Marathon\nKubernetes 1.x\nRancher\nDocker\nAzure Container Service\n\nConfiguration\n\nConfigure Vamp\n\n Try Vamp\n\nWe've put together a hello world walkthrough to let you try out some of Vamp's core features in a local docker container. You can use this to work through the getting started tutorials.\n\n","id":18},{"path":"/documentation/installation/kubernetes","date":"2016-10-04T09:00:00+00:00","title":"Kubernetes 1.x","content":"The installation will run Vamp together with etcd, Elasticsearch and Logstash on Google container engine and kubernetes. (We will also deploy our demo Sava application to give you something to play around on). Before you begin, it is advisable to try out the official Quickstart for Google Container Engine tutorial first (google.com - container engine quickstart).  \n\n{{ note title=\"Note!\" }}\nKubernetes support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Kubernetes 1.2 and 1.3. Minikube can also be used. (github.com - minikube)\n\n Requirements\n\nGoogle Container Engine cluster\nKey-value store (like ZooKeeper, Consul or etcd)\nElasticsearch and Logstash\n\nStep 1: Create a new GKE cluster:\n\nThe simple way to create a new GKE cluster:\n\nopen Google Cloud Shell\nset a zone, e.g. gcloud config set compute/zone europe-west1-b\ncreate a cluster vamp using default parameters: gcloud container clusters create vamp\n\nAfter the (new) Kubernetes cluster is setup, we are going to continue with the installation using the Kubernetes CLI kubectl.\nYou can use kubectl directly from the Google Cloud Shell, e.g. to check the Kubernetes client and server version:\n\n$ kubectl version\n\n Step 2: Deploy etcd, Elasticsearch and Logstash\n\nLet's deploy etcd - the installation is based on this tutorial (github.com/coreos - etcd on Kubernetes).\nExecute:\n\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/etcd.yml\n\nDeploy Elasticsearch and Logstash with a proper Vamp Logstash configuration (github.com/magneticio - elastic):\n\n$ kubectl run elastic --image=magneticio/elastic:2.2\n$ kubectl expose deployment elastic --protocol=TCP --port=9200 --name=elasticsearch\n$ kubectl expose deployment elastic --protocol=UDP --port=10001 --name=logstash\n$ kubectl expose deployment elastic --protocol=TCP --port=5601 --name=kibana\n{{ note title=\"Note!\" }}\nThis is not a production grade setup. You would also need to take care of persistence and running multiple replicas of each pod.\n{{ /note }}\n\nStep 3: Run Vamp\n\nLet's run Vamp gateway agent as a daemon set first:\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/vga.yml\n\nTo deploy Vamp, execute:\n\n$ kubectl run vamp --image=magneticio/vamp:0.9.0-kubernetes\n$ kubectl expose deployment vamp --protocol=TCP --port=8080 --name=vamp --type=\"LoadBalancer\"\n\nThe Vamp image uses the following configuration (github.com/magneticio - Vamp kubernetes configuration). The Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nWait a bit until Vamp is running and check out the Kubernetes services:\n\n$ kubectl get services\n\nThe output should be similar to this:\n\nNAME                 CLUSTER-IP     EXTERNAL-IP      PORT(S)             AGE\nelasticsearch        10.3.242.188   none           9200/TCP            4m\netcd-client          10.3.247.112   none           2379/TCP            4m\netcd0                10.3.251.13    none           2379/TCP,2380/TCP   4m\netcd1                10.3.251.103   none           2379/TCP,2380/TCP   4m\netcd2                10.3.250.20    none           2379/TCP,2380/TCP   4m\nkubernetes           10.3.240.1     none           443/TCP             5m\nlogstash             10.3.254.16    none           10001/UDP           4m\nvamp                 10.3.242.93    146.148.118.45   8080/TCP            2m\nvamp-gateway-agent   10.3.254.234   146.148.22.145   80/TCP              2m\n\nNotice that the Vamp UI is exposed (in this example) on http://146.148.118.45:8080\n\n Step 4: Deploy the Sava demo application\n\n,name: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\n{{ note title=\"Note!\" }}\nBe sure that the cluster has enough resources (CPU, memory), otherwise deployments will be in pending state.\n{{ /note }}\n\nOnce it's running we can check if all Vamp Gateway Agent services are up:\n\n$ kubectl get services --show-labels -l vamp=gateway\n\nWe can see that for each gateway a new service is created:\n\nNAME                      CLUSTER-IP     EXTERNAL-IP     PORT(S)     AGE       LABELS\nhex1f8c9a0157c9fe3335e9   10.3.243.199   104.155.24.47   9050/TCP    2m        lookup_name=a7ad6869e65e9c047f956cf7d1b4d01a89e\nef486,vamp=gateway\nhex26bb0695e9a85ec34b03   10.3.245.85    23.251.143.62   40000/TCP   2m        lookup_name=6ace45cb2c155e85bd0c84123d1dab5a6cb\n12c97,vamp=gateway\n\n{{ note title=\"Note!\" }}\nIn this setup Vamp is deliberately configured to initiate exposure of all gateway and VGA ports. This would not be the case if the default and recommended setting are used.\n{{ /note }}\n\nNow we can access our sava service on http://104.155.24.47:9050\n\nThe default Kubernetes service type can be set in configuration: vamp.container-driver.kubernetes.service-type, possible values are LoadBalancer or NodePort.\n\nWe can also access gateways using virtual hosts. Vamp Gateway Agent service is on IP 146.148.22.145 in this example, so:\n$ curl --resolve 9050.sava-1-0.vamp:80:146.148.22.145 -v http://9050.sava-1-0.vamp\n\n{{ note title=\"Note!\" }}\nDon't forget to clean up your Kubernetes cluster and firewall rules  if you don't want to use them anymore (google.com - container engine quickstart: clean up).\n{{ /note }}\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}\n","id":19},{"path":"/documentation/installation/mesos-marathon","date":"2016-09-30T12:00:00+00:00","title":"Mesos/Marathon","content":"\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nInstall\nThe instructions included on the DC/OS installation page will also work with Mesos/Marathon.\n\n set container driver\nSee set Mesos/Marathon as the container driver\n\n","id":20},{"path":"/documentation/installation/rancher","date":"2016-09-13T09:00:00+00:00","title":"Rancher","content":"This installation will run Vamp together with Consul, Elasticsearch and Logstash on Rancher. (We'll also deploy our demo Sava application to give you something to play around on). Before you begin, it is advisable to try out the official Rancher Quick Start Guide tutorial first (rancher.com - quick start guide).\n\n{{ note title=\"Note!\" }}\nRancher support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Rancher version 1.1.x.\n\n Requirements\n\nRancher up and running\nKey-value store like ZooKeeper, Consul or etcd\nElasticsearch and Logstash\nIf you want to make a setup on your local VM based Docker, it's advisable to increase default VM memory size from 1GB to 4GB.\n\nStep 1: Run Rancher locally\nBased on the official Rancher quickstart tutorial, these are a few simple steps to run Rancher locally:\n$ docker run -d --restart=always -p 8080:8080 rancher/server\nThe Rancher UI is exposed on port 8080, so go to http://SERVER_IP:8080 - for instance http://192.168.99.100:8080, http://localhost:8080 or something similar depending on your Docker setup.\n\nFollow the instructions on the screen to add a new Rancher host:\n\nclick on \"Add Host\" and then on \"Save\".\nYou should get instructions (bullet point 5) to run an agent Docker image:  \n\n$ docker run \\\n  -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /var/lib/rancher:/var/lib/rancher \\\n  rancher/agent:v1.0.1 \\\n  http://192.168.99.100:8080/v1/scripts/E78EF5848B989FD4DA77:1466265600000:SYqIvhPgzKLonp8r0erqgpsi7pQ\n\n Step 2: Run Vamp stack\nNext we need to create a Vamp stack. This can be done either from catalog or from scratch (adding all dependencies manually - Consul, Elasticsearch, Logstash).\n\nRun Vamp stack from catalog\n\nGo to Catalog\nFind the Vamp entry, click the Details button\nGo to Step 3: Run Vamp\n\n Run Vamp stack from scratch\n\nGo to Add Stack and create a new stack vamp (lowercase).\nInstall Consul:  \n  Use the vamp stack and go to Add Service:  \n    Name ⇒ consul\n    Select Image ⇒ gliderlabs/consul-server\n    Set Command ⇒ -server -bootstrap\n    Go to Networking tab\n    Under Hostname select Set a specific hostname: and enter consul\n    Click the Create button\n\nInstall Elasticsearch and Logstash:\n  Use the vamp stack and go to Add Service:  \n    Name ⇒ elastic\n    Select Image ⇒ magneticio/elastic:2.2\n    Go to Networking tab\n    Under Hostname select Set a specific hostname: and enter elastic\n    Click the Create button\n\n Our custom Docker image magneticio/elastic:2.2 contains Elasticsearch, Logstash and Kibana with the proper Logstash configuration for Vamp. More details can be found on the github project page (github.com/magneticio - elastic).\n\nStep 3: Run Vamp\n\nFirst we'll run the Vamp Gateway Agent:\n  Use the vamp stack and go to Add Service:\n  Set scale to Always run one instance of this container on every host\n  Name ⇒ vamp-gateway-agent\n  Select Image ⇒ magneticio/vamp-gateway-agent:0.9.1\n  Set Command ⇒ --storeType=consul --storeConnection=consul:8500 --storeKey=/vamp/gateways/haproxy/1.6 --logstash=elastic:10001\n  Go to Networking tab\n  Under Hostname select Set a specific hostname: and enter vamp-gateway-agent\n  Click on Create button\n\nNow let's find a Rancher API endpoint that can be accessed from running container:\n  Go to the API page and find the endpoint, e.g. http://192.168.99.100:8080/v1/projects/1a5\n  Go to the Infrastructure/Containers and find the IP address of rancher/server, e.g. 172.17.0.2\n  The Rancher API endpoint should be then http://IP_ADDRESS:PORT/PATH based on values we have, e.g. http://172.17.0.2:8080/v1/projects/1a5\n\nNow we can deploy Vamp:\n  Use the vamp stack and go to Add Service:\n  Name ⇒ vamp\n  Select Image ⇒ magneticio/vamp:0.9.1-rancher\n  Go to Add environment variable VAMPCONTAINERDRIVERRANCHERURL with value of Rancher API endpoint, e.g. http://172.17.0.2:8080/v1/projects/1a5\n  (optional) add a VAMPCONTAINERDRIVERRANCHERUSER variable with a Rancher API access key if your Rancher installation has access control enabled\n  (optional) add a VAMPCONTAINERDRIVERRANCHERPASSWORD variable with a matching Rancher API secret key.\n  Go to Networking tab\n  Under Hostname select Set a specific hostname: and enter vamp\n  Click the Create button\n  Go to Add Load Balancer (click arrow next to Add Service button label)\n  Choose a name (e.g. vamp-lb)\n  Source IP/Port ⇒ 9090\n  Default Target Port ⇒ 8080 and Target Service ⇒ vamp\n\nIf you go to http://SERVER_IP:9090 (e.g http://192.168.99.100:9090), you should get the Vamp UI.  The Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.  \nYou should also notice that Vamp Gateway Agent is running (one instance on each node) and you can see some Vamp workflows running.\n\nTo access HAProxy stats:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose a name (e.g. vamp-gateway-agent-lb), Source IP/Port ⇒ 1988, Default Target Port ⇒ 1988 and Target Service ⇒ vamp-gateway-agent\nUse the following username/password: haproxy for the HAProxy stats page\n\n Step 4: Deploy the Sava demo application\n\nLet's deploy our sava demo application:\n\n,name: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nIf you want the gateway port to be exposed outside of the cluster via a Rancher Load Balancer:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose name (e.g. gateway-9050),\n  Source IP/Port ⇒ 9050\n  Default Target Port ⇒ 9050\n  Target Service ⇒ vamp-gateway-agent\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}\n","id":21},{"path":"/documentation/release notes/katana","date":"2016-10-19T09:00:00+00:00","title":"katana","content":"\n{{ note title=\"katana is not an official release\"}}\nAll changes since the last official release are described below. This applies only to binaries built from source (master branch). \n{{ /note }}\n\nWhat has changed\nno changes since last release.\n\n What is new\nno changes since last release.\n\n{{ note title=\"What next?\" }}\nRead all release notes on github (github.com/magneticio - Vamp releases)\nTry out Vamp with our single container hello world package.\n{{ /note }}\n","id":22},{"path":"/documentation/release notes/older-versions","date":"2016-11-02T09:00:00+00:00","title":"Older versions","content":"\nAll release notes about older Vamp versions can be found in the Github release notes.\n","id":23},{"path":"/documentation/release notes/version-0-9-0","date":"2016-10-19T09:00:00+00:00","title":"version 0.9.0","content":"\n9th September 2016\n\nThe Vamp 0.9.0 release is a very important milestone in the lifecycle of Vamp, as we're removing the Alpha label and are moving to Beta! This means that we will do our utmost best to avoid breaking changes in our API's and DSL, focus even more on stabilising and optimising the current feature-set, while of course continuously introducing powerful new features.\n\nThe Vamp 0.9.0 release is the culmination of three months of hard work by our amazing team! This release incorporates nothing less than 115 issues and I'm very proud of what we've achieved.\n\nSome of the most notable new features are:\n\na brand new opensource UI with much better realtime graphs, sparklines, info, events panel and access to all relevant API objects like breeds, deployments but also new options like gateways and workflows.  \npowerful integrated workflows for automation and optimisation like autoscaling, automated canary-releasing etc. using efficient Javascript-based scripting.\nKubernetes and Rancher support.\nsupport for custom virtual host names in gateways.\nsupport for custom HAProxy templates.\na brand new Vamp Runner helper application for automated integration testing, mocking scenarios and educational purposes.  \n\nAnd, of course, there's a massive amount of improvements, bug fixes and other optimisations.\n\ngithub.com/magneticio - complete list of all the closed issues in this release\n\n{{ note title=\"What next?\" }}\nRead all release notes on github (github.com/magneticio - Vamp releases)\nYou can try out Vamp with our single container hello world package.\n{{ /note }}\n","id":24},{"path":"/documentation/release notes/version-0-9-1","date":"2016-10-01T09:00:00+00:00","title":"version 0.9.1","content":"\n1st November 2016\n\nThe new stuff\nThis release of Vamp introduces:\n\nThe biggie: We've added Websockets support to our HTTP API. And we're now using this heavily in our new UI to improve responsiveness, smoothness and speed. https://github.com/magneticio/vamp/issues/529\nWe've updated our UI to a nice dark theme due to public demand, we love it as it's much easier on the eyes, and of course we're very interested in hearing your thoughts!\nWe've updated our charts with the amazing Smoothie Charts library for smooth running charts and sparklines.\nYou can now configure Vamp to use a key-value store for persistence data storage. By default nothing is defined, and you need to choose either ElasticSearch or key-value. Take a look at the Vamp Quickstart configuration for possible settings. The design reasons for this addition are having less dependencies on Elasticsearch, better re-use of the available key-value stores that come with cluster-managers (like Zookeeper in DC/OS or Etcd in Kubernetes) and more robustness (i.e. if we temporarily loose ES the persistence data is still available in the K/V store, only the metrics data is temporarily unavailable). Possible issues might be the performance of the key-value store after some time. This is a known issue being investigated. https://github.com/magneticio/vamp/issues/750\nGateway stickyness is now editable through the UI\nScales and gateways are seperate entities exposed through the UI\nMulti-select delete actions in the UI\nYou can now filter health and/or metrics events in the EVENTS stream panel\nAnd of course lots of other improvements and bug-fixes that can be found here: https://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.9.1+is%3Aclosed\n\n What has changed\nBREAKING CHANGE: In the Vamp configuration the “rest-api” section has changed to “http-api”. When running Vamp 0.9.1 you need to change this setting accordingly. NB REST and websockets are both a part of our HTTP API. Check this Vamp configuration example.\nBREAKING CHANGE: The default value for Vamp Gateway Agent storeKey has /vamp/gateways/haproxy/1.6 - changed from /vamp/haproxy/1.6\nIn the Vamp configuration we set persistence caching by default to false. In our pre-build Vamp images we set this to true to make it easier on the persistence store load. https://github.com/magneticio/vamp/issues/792\nWe've changed the updating deployment service states. https://github.com/magneticio/vamp/issues/797\n\nKnown issues\nHealth in deployment detail screen stays at 100% even when a gateway error is measured. In the gateway detail screen this works correctly. \nMemory leak issues when running workflows.\nInstability when using Zookeeper in the Vamp quickstart packages after a period of time.\nWhen using Elasticsearch for persistence storage there can be stale data due to speed of the websockets implementation and slower / eventual indexing of ES. A browser refresh solves this.\nVamp-docker quickstart throws error when starting up.\n\nThe full list of improvements and bug-fixes that can be found here: https://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.9.1+is%3Aclosed\n\n{{ note title=\"What next?\" }}\nRead all release notes on github (github.com/magneticio - Vamp releases)\nYou can try out Vamp with our single container hello world package.\n{{ /note }}\n","id":25},{"path":"/documentation/tutorials/deploy-your-first-blueprint","date":"2016-09-13T09:00:00+00:00","title":"Deploy your first blueprint","content":"If everything went to plan, you should have your Vamp installation up and running. If not, please follow the Vamp hello world quick setup steps. Now we're ready to check out some of Vamp's features. In this tutorial we will:  \n\nDeploy a monolith, using either the Vamp UI or the Vamp API\nCheck out the deployed application  \nGet some metrics on the running application  \nChange the scale and load-balancing\nChaos monkey!    \n\nDeploy a monolith\n\nImagine you or the company you work for still use monolithic applications. I know, it sounds far fetched...\nThis application is conveniently called Sava monolith and is at version 1.0.  \n\nYou've managed to wrap your monolith in a Docker container, which lives in the Docker hub under magneticio/sava:1.0.0. Your app normally runs on port 8080 but you want to expose it under port 9050 in this case. Let's deploy this through Vamp using the following simple blueprint. Don't worry too much about what means what: we'll get there. You can choose to deploy this blueprint either using the Vamp UI or using the Vamp API.\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\n Deploy using the Vamp UI\n\nIn the Vamp UI, go to the BLUEPRINTS tab and click the ADD button (top right). Paste in the above blueprint and press the SAVE button. Vamp will store the blueprint and make it available for deployment. \nNow Press DEPLOY AS. You'll be prompted to give your deployment a name, let's call it sava, then press DEPLOY to start the deployment and you can skip to Check out the deployed application.\n\nDeploy using the Vamp API\n\nYou can use your favourite tools like Postman (getpostman.com), HTTPie (github.com/jakubroztocil - httpie) or Curl to post this blueprint directly to the api/v1/deployments endpoint of Vamp.\n\nTake care to set the correct Content-Type: application/x-yaml header on the POST request. Vamp is kinda\nstrict with regard to content types, because we support JSON and YAML so we need to know what you are sending.   \nIf you run on Docker machine, use docker-machine ip default instead of localhost.\n\nUsing curl\ncurl -v -X POST --data-binary @sava_1.0.yaml -H \"Content-Type: application/x-yaml\" http://localhost:8080/api/v1/deployments\n\nUsing httpie\nhttp POST http://localhost:8080/api/v1/deployments Content-Type:application/x-yaml < sava_1.0.yaml\n\nAfter POST-ing, Vamp should respond with a 202 Accepted message and return a JSON blob. This means Vamp is trying to deploy your container. You'll notice some parts are filled in for you, like a default scale, a default routing and of course a UUID as a name.\nYou can also use the RESTful API to create a deployment with a custom name - simple PUT request to http://localhost:8080/api/v1/deployments/DEPLOYMENTCUSTOMNAME\n\n Check out the deployed application \n\nYou can follow the deployment process of our container by checking the /api/v1/deployments endpoint and checking when the state field changes from ReadyForDeployment to Deployed. You can also check Marathon's GUI.\n\nWhen the application is fully deployed you can check it out at Vamp host address + the port that was assigned in the blueprint, e.g: http://10.26.184.254:9050/. It should report a refreshing hipster lorem ipsum (hipsterjesus.com) upon each reload.  \n\nSee check Vamp is up and running for a full ist of all services exposed in the hello world setup.\n\nGet some metrics on the running application\n\nWe can use a simple tool like Apache Bench (apache - ab) to put some load on our application and see some of the metrics flowing into the dashboard. Use the following command to send 10000 requests using 15 threads to our Sava app.\n\nab -k -c 15 -n 10000 http://localhost:9050/\nor\nab -k -c 15 -n 10000 http://docker-machine ip default:9050/\n\nYou should see the metrics spike and some pretty charts being drawn:\n\n Change scale and load-balancing\n\nVamp will automatically load-balance services. Let's change the scale of the service. Go to the DEPLOYMENTS tab and open the sava deployment. Click the edit icon under SCALE and enter 3** in the **instances field. click SAVE and Vamp will automatically scale up the number of running instances (of course permitting underlying resources) and load-balance these to the outside world using the gateway feature.\n\nChaos monkey\n\nNow let's try something fun. Go to the Marathon UI (on port 9090) and find the Sava container running. Now select destroy to kill the container. Watch Vamp detecting that issue and making sure that the defined number of instances is spun up again as soon as possible, while making sure the loadbalancing routing rules are also updated to reflect the changed IPs and ports of the instances.\n\n{{ note title=\"What next?\" }}\nLet's run a canary release in the second part of this getting started tutorial →\n{{ /note }}\n\n","id":26},{"path":"/documentation/tutorials/","date":"2016-09-13T09:00:00+00:00","title":"Tutorials","content":"\nGetting started\nWe’ve created a set of showcase applications, services and corresponding blueprints that demonstrate Vamp’s core features - together we call them “Sava”. Sava is a mythical vampire from Serbia (wiki), but in our case it is a Github repo full of examples to help us demonstrate Vamp.\nYou can work with Sava in the Vamp hello world setup (or any other Vamp installation).\n\nDeploy your first blueprint\nRun a canary release\nSplit a monolith into services\nMerge a changed topology\n","id":27},{"path":"/documentation/tutorials/merge-and-delete","date":"2016-09-13T09:00:00+00:00","title":"Merge and delete","content":"\nIn the previous tutorial we \"over-engineered\" our service based solution a bit (on purpose of course). We don't really need two backends services, so in this tutorial we will introduce our newly engineered solution and transition to it using Vamp blueprints and canary releasing methods. In this tutorial we will:\n\nGet some background and theory on merging services\nPrepare our blueprint\nTransition from blueprints to deployments (and back)\nDelete parts of the deployment\nAnswer the all important question when would I use this?\n\nSome background and theory\n\nWhat we are going to do is create a new blueprint that is completely valid by itself and merge it\nwith our already running deployment. This might sound strange at first, but it makes sense. Why? Merging will enable us to slowly move from the previous solution to the next solution. Once moved over, we can\nremove any parts we no longer need, i.e. the former \"over-engineered\" topology.\n\nIn the diagram above, this is visualized as follows:\n\nWe have a running deployment (the blue circle with the \"1\"). To this we introduce a new blueprint\nwhich is merged with the running deployment (the pink circle with the \"2\").\nAt a point, both are active as we are transitioning from blue to pink.\nOnce we are fully on pink, we actively remove/decommission the blue part.\n\nIs this the same as a blue/green release? Yes, but we like pink better ;o)\n\n Prepare our blueprint\n\nThe below blueprint describes our more reasonable service topology. Again, this blueprint is completely\nvalid by itself. You could just deploy it somewhere separately and not merge it with our over-engineered\ntopology. Notice the following:\n\nThe blueprint only has one backend cluster with one service.\nThe blueprint does not specify a gateway using the gateways key because we are going to use the gateway already present and configured in the running deployment. However, it would be perfectly correct to specify the old gateway - the gateway would be updated as well.\n\n,name: sava:1.3\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        dependencies:\n          backend: sava-backend:1.3.0\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n  backend:\n    services:\n      breed:\n        name: sava-backend:1.3.0\n        deployable: magneticio/sava-backend:1.3.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nUpdating our deployment using the UI or a PUT to the /api/v1/deployments/{deployment_name} should yield a deployment with the following properties (we left some irrelevant\nparts out):\n\nTwo services in the sava cluster: the old one at 100% and the new one at 0% weight.\nThree backends in the cluster list: two old ones and one new one.\n\nSo what happened here? Vamp has worked out what parts were already there and what parts should be merged or added. This is done based on naming, i.e. the sava cluster already existed, so Vamp added a service to it at 0% weight. A cluster named \"backend\" didn't exist yet, so it was created. Effectively, we have merged\nthe running deployment with a new blueprint.\n\nTransition from blueprints to deployments and back\n\nMoving from the old to the new topology is now just a question of \"turning the weight dial\". You\ncould do this in one go, or slowly adjust it. The easiest and neatest way is to just update the deployment as you go.\n\nVamp's API has a convenient option for this: you can export any deployment as a blueprint! By appending ?as_blueprint=true to any deployment URI, Vamp strips all runtime info and outputs a perfectly valid blueprint of that specific deployment.\n\nThe default output will be in JSON format, but you can also get a YAML format. Just set the header Accept: application/x-yaml and Vamp will give you a YAML format blueprint of that deployment.\n\nWhen using the graphical UI, this is all taken care of.\n\nIn this specific example, we could export the deployment as a blueprint and update the weight to a 50% to\n50% split. Then we could do this again, but with a 80% to 20% split and so on. See the abbreviated example\nbelow where we set the weight keys to 50% in both routing sections.\n\n,name: eb2d505e-f5cf-4aed-b4ae-326a8ca54577\nclusters:\n  sava:\n    services:\n    breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        constants: {}\n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      environment_variables: {}\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: {}\n    breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        constants: {}\n        dependencies:\n          backend: sava-backend:1.3.0\n      environment_variables: {}\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: {}\n    gateways:\n      port:\n        sticky: none\n        routes:\n          sava-frontend:1.2.0:\n            weight: 50%\n          sava-frontend:1.3.0:\n            weight: 50%\n\n Delete parts of the deployment\n\nVamp helps you transition between states and avoid \"hard\" switches, so deleting parts of a deployment is somewhat different than you might expect.\n\nIn essence, a delete is just another update of the deployment: you specify what you want to remove using a blueprint and send it to the deployment's URI using the DELETEHTTP verb: yes, it is HTTP Delete with a body, not just a URI and some id.\n\nThis means you can specifically target parts of your deployment to be removed instead of deleting the whole thing. For this tutorial we are going to delete the \"over-engineered\" old part of our deployment.\n\nCurrently, deleting works in two steps:\nSet all routings to weight: 0% of the services you want to delete with a simple update.\nExecute the delete.\n\n{{ note title=\"Note!\" }}\nYou need to explicitly set the routing weight of the service you want to deploy to zero before deleting. Here is why: When you have, for example, four active services divided in a 25/25/20/30 split and you delete the one with 30%, Vamp doesn't know how you want to redistribute the \"left over\" 30% of traffic. For this reason the user should first explicitly divide this and then perform the delete.\n{{ /note }}\n\nSetting to zero\n\nWhen you grab the YAML version of the deployment, just like above, you can set all the weight entries for the Sava 1.2.0 versions to 0 and update the deployment as usual. See the cleaned up example and make sure to adjust the name to your specific situation.\n\n,name: 125fd95c-a756-4635-8e1a-361085037870\nclusters:\n  backend1:\n    services:\n    breed:\n        ref: sava-backend1:1.2.0\n    gateways:\n      routes:\n        sava-backend1:1.2.0:\n          weight: 0%\n  backend2:\n    services:\n    breed:\n        ref: sava-backend2:1.2.0\n    gateways:\n      routes:\n        sava-backend2:1.2.0:\n          weight: 0%\n  sava:\n    services:\n    breed:\n        ref: sava-frontend:1.3.0\n\n    breed:\n        ref: sava-frontend:1.2.0\n    gateways:\n      routes:\n        sava-frontend:1.3.0:\n          weight: 100%\n        sava-frontend:1.2.0:\n          weight: 0%\n\nDoing the delete\n\nNow, you can take the exact same YAML blueprint or use one that's a bit cleaned up for clarity and send it in the body of the DELETE to the deployment resource, e.g. /api/v1/deployments/125fd95c-a756-4635-8e1a-361085037870.\n\nUsing the Vamp UI you can delete parts of your deployment by using the REMOVE FROM function under the BLUEPRINTS tab.\n\n,name: sava:1.2\nclusters:\n  sava:\n    services:\n      breed:\n        ref: sava-frontend:1.2.0\n  backend1:\n    services:\n      breed:\n        ref: sava-backend1:1.2.0\n  backend2:\n    services:\n      breed:\n        ref: sava-backend2:1.2.0\n\nWe removed the deployable, environment_variables, ports and some other parts of the blueprint. These are actually not necessary for updating or deletion. Besides that, this is actually exactly the same blueprint we used to initially deploy the \"old\" topology.\n\nYou can check the result in the UI: you should be left with just one backend and one frontend:\n\nWhen would I use this?\n\nSounds cool, but when would I use this in practice? Well, basically anytime you release something new!\nFor example a bugfix release for a mobile API that \"didn't change anything significantly\"? You could test\nthis separately and describe it in its own blueprint. After testing, you would merge that exact same blueprint\nwith your already running production version (the one without the bugfix) and slowly move over to new version.\n\nNew major release of your customer facing app? You probably also have some new dependencies that come with that\nrelease. You create some containers and write up a blueprint that describes this new situation, run it in acceptance and test and what have you. Later, you merge it into your production setup, effectively putting it next to it and then slowly move from the old situation to the new situation, including dependencies.\n\n{{ note title=\"What next?\" }}\nThis is the end of this initial getting started tutorial. We haven't done anything with Vamp's SLA's yet, scaling or dictionary system, so there is much more to come!\nVamp use cases\nFind out how to install a production-grade set up of Vamp\n{{ /note }}\n\n","id":28},{"path":"/documentation/tutorials/run-a-canary-release","date":"2016-09-13T09:00:00+00:00","title":"Run a canary release","content":"\nIn the previous tutorial we deployed our app sava 1.0. If you haven't walked through that part already, please do so before continuing.\n\nNow let's say we have a new version of this great application that we want to canary release into production. We have it containerised as magneticio/sava:1.1.0 and are ready to go. In this tutorial we will:\n\nPrepare our blueprint\nDeploy the new version of our application next to the old one\nUse conditions to target specific groups\nLearn a bit more about conditions\n\nPrepare our blueprint\n\nVamp allows you to do canary releases using blueprints. Take a look at the YAML example below. It is quite similar to the blueprint we initially used to deploy sava 1.0.0. However, there are two big differences.\n\nThe services key holds a list of breeds: one for v1.0.0 and one for v1.1.0 of our app. Breeds are Vamp's way of describing static artifacts that can be used in blueprints.\nWe've added the routes key which holds the weight of each service as a percentage of all requests. Notice we assigned 50% to our current version 1.0.0 and 50% to the new version 1.1.0 We could also start with a 100% to 0% split, a 99% to 1% split or whatever combination you want as long as all percentages add up to 100% in total. There is nothing stopping you from deploying three or more versions and distributing the weight among them. Just make sure that when doing a straight three-way split you give one service 34% as 33%+33%+33%=99%.\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 50%   weight in percentage\n        sava:1.1.0:\n          weight: 50%\n    services: # services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\nYou could also just leave out the whole routes section and use the UI to change the weights after we've done the deployment.\n\nDeploy the new version of our application next to the old one\n\nIt is our goal to update the already running deployment with the new blueprint. Vamp will figure out that v1.0.0\nis already there and just add v1.1.0 while setting the correct routing between these services. You could also create a second blueprint with the new service, and merge this new service to the Sava-cluster so it becomes available for routing traffic to it.\n\n Deploy using the UI\n\nGo to the DEPLOYMENTS tab, open the running deployment and click the EDIT button (top right). Copy the blueprint above and paste it over the the deployment that is there then click SAVE. Vamp will start working out the differences and update the deployment accordingly.\n\nDeploy using the API\n\nGet the running deployment's name (the UUID) from /api/v1/deployments (or use the explicit name that you used for the deployment) and PUT the blueprint to that resource, e.g: /api/v1/deployments/e1c99ca3-dc1f-4577-aa1b-27f37dba0325\n\n Check the deployment and routing\nWhen Vamp has finished deploying, you can start refreshing your browser at the correct endpoint, e.g. http://192.168.99.100:9050/. The application should switch between responding with a 1.0 page and a 1.1 page. Note that this works best with the \"Incognito\" or \"Anonymous\" mode of your browser because of the caching of static assets.\n\nUse conditions to target specific groups\n\nUsing percentages to divide traffic between versions is already quite powerful, but also very simplistic.\nWhat if, for instance, you want to specifically target a group of users? Or a specific channel of requests\nfrom an internal service? Vamp allows you to do this right from the blueprint DSL.\n\nLet's start simple: We will allow only Chrome users to access v1.1.0 of our application by inserting this routing scheme:\n\n,routes:\n  sava:1.1.0:\n    weight: 0%\n    filter_strength: 100%\n    filters:\n    condition: User-Agent = Chrome\n\nNotice three things:\n\nWe inserted a list of conditions (with only one condition for now).\nWe set the filter strength to 100% (it would be also by default set to 100%). This is important because we want all Chrome users to access the new service - we could also say filter_strength: 50% to give access just to half of them (the other 50% would be redirected to weight rules and routed accordingly).\nWe set the weight to 0% because we don't want any other users to access sava:1.1.0\n\nThe first service where the filter matches the request will be used to handle the request.\nMore information about using filters, weights, sticky sessions etc..  \n\nOur full blueprint now looks as follows:\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 100%\n        sava:1.1.0:\n          weight: 0%\n          condition_strength: 100%\n          condition: User-Agent = Chrome\n    services:  services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n\nUsing the UI, you can either use the EDIT button from the deployment details screen again and completely paste in this blueprint or just\nfind the right place in the blueprint and edit it by hand. The result should be the same as using our UI to insert a filter condition:\n\nAs we are not actually deploying anything but just reconfiguring routes, the update should be almost instantaneous. You can fire up a Chrome browser and a Safari browser and check the results. A hard refresh might be necessary because of your browser's caching routine.\n\nA bit more about conditions\n\nOur browser example is easily testable on a laptop, but of course a bit contrived. Luckily you can\ncreate much more powerful filters quite easily. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv/configuration-1.5 - ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a filter.\n\n Vamp short codes\n\nACLs can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following, but we will be expanding on this in the future:\n\nUser-Agent = string\nHost = string\nCookie cookie name Contains string\nHas Cookie cookie name\nMisses Cookie cookie name\nHeader header name Contains string\nHas Header header name\nMisses Header header name\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nhdr_sub(user-agent) Android   # straight ACL\nuser-agent=Android            # lower case, no white space\nUser-Agent=Android            # upper case, no white space\nuser-agent = Android          # lower case, white space\n\nMultiple conditions in a filter\nHaving multiple conditions in a filter is perfectly possible, all filters will be implicitly\n\"OR\"-ed together, as in \"if the first filter doesn't match, proceed to the next\".   \n\nIn the example below, the filter would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest. If that doesn't result in a match, it would check whether the request has the header\n\"X-VAMP-TUTORIAL\". So any request matching either condition would go to this service.\n\n,routes:\n  sava:1.1.0:\n    filter_strength: 100%\n    filters:\n    condition: User-Agent = Chrome\n    condition: Has Header X-VAMP-TUTORIAL\n\nUsing a tool like httpie (github.com/jakubroztocil - httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-TUTORIAL:stuff\n\n{{ note title=\"What next?\" }}\nCool stuff. But we are dealing here with single, monolithic applications. Where are the microservices?  We will chop up this monolith into services and deploy them with Vamp in the third part of our tutorial →\n{{ /note }}\n","id":29},{"path":"/documentation/tutorials/split-a-monolith","date":"2016-09-13T09:00:00+00:00","title":"Split a monolith","content":"In the previous tutorial we did some basic canary releasing on two versions of a monolithic application. Very nice, but Vamp isn't\ncalled the Very Awesome Microservices Platform for nothing. The next step is to split our monolithic Sava application into separate services. In this tutorial we will:\n\ndefine a new service topology\nlearn about environment variables and service discovery\n\nDefine a new service topology\n\nTo prove our point, we are going to slightly \"over-engineer\" our services solution. This will also help\nus demonstrate how we can later remove parts of our solution using Vamp. For now, we'll split the\nmonolith into a topology of one frontend and two separate backend services. After our engineers\nare done with coding, we can catch this new topology in the following blueprint. Please notice a couple\nof things:\n\nWe now have three clusters: sava, backend1 and backend2. Each cluster could have multiple\nservices on which we could do separate canary releases and set separate filters.\nThe sava cluster has explicit dependencies on the two backends. Vamp will make sure these dependencies\nare checked and rolled out in the right order.\nUsing environment_variables we connect the dynamically assigned ports and hostnames of the backend\nservices to the \"customer facing\" sava service.\nWe've change the gateway port to 9060 so it doesn't collide with the  monolithic deployment.\n\n,name: sava:1.2\ngateways:\n  9060: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http                \n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      scale:\n        cpu: 0.2      \n        memory: 64MB\n        instances: 1               \n  backend1:\n    services:\n      breed:\n        name: sava-backend1:1.2.0\n        deployable: magneticio/sava-backend1:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1              \n  backend2:\n    services:\n      breed:\n        name: sava-backend2:1.2.0\n        deployable: magneticio/sava-backend2:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\nDeploy this blueprint using either the UI or a REST call and when deployed check out the new topology in your browser (on port 9060 this time). When deployed it should yield something similar to:\n\n Learn about environment variables and service discovery\n\nIf you were to check out the Docker containers using docker inspect, you would see the environment variables that we set in the blueprint.\n\n docker inspect 66e64bc1c8ca\n...\n\"Env\": [\n    \"BACKEND_1=http://172.17.42.1:33021/api/message\",\n    \"BACKEND_2=http://172.17.42.1:33022/api/message\",\n    \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n],\n...\n\nHost names and ports are configured at runtime and injected in the right parts of your running deployment. Your service/app should pick up these variables to configure itself. Luckily, this is quite easy and common in almost all languages and frameworks.\n\nRemember, there is no \"point-to-point\" wiring. The exposed host and port are actually service\nendpoints. The location, amount and version of containers running behind that service endpoint can vary.\nLearn more about how Vamp does service discovery.\n\n{{ note title=\"What next?\" }}\nGreat! We just demonstrated that Vamp can handle dependencies between services and configure these services with host and port information at runtime. Now let's do a more complex migration to a new service based topology →.\n{{ /note }}\n\n","id":30},{"path":"/documentation/using vamp/artifacts","date":"2016-09-13T09:00:00+00:00","title":"Artifacts","content":"\nVamp has a few basic entities or artifacts you can work with, these can be classed as static resource descriptions and dynamic runtime entities. Note that API actions on static resource descriptions are mostly synchronous, while API actions on dynamic runtime entities are largely asychronous.\n\nStatic resource descriptions\n\nBreeds describe single services and their dependencies.  Read more...\nBlueprints are, well, blueprints! They describe how breeds work in runtime and what properties they should have.  Read more...  \n\n Dynamic runtime entities\n\nDeployments are running blueprints. You can have many deployments from one blueprint and perform actions on each at runtime. Plus, you can turn any running deployment into a blueprint.  Read more...  \nGateways are the \"stable\" routing endpoint - defined by a port (incoming) and routes (outgoing).  Read more... \nWorkflows are apps (services) deployed on cluster, used for dynamically changing the runtime configuration (e.g. SLA, scaling, condition weight update).  Read more...\n\nWorking across multiple teams\n\nIn larger companies with multiple teams working together on a large project, all required information is often not available at the same time. To facilitate this style of working, Vamp allows you to set placeholders. Placeholders let you communicate with other teams using simple references and gradually build up a complicated deployment. Vamp will only check references at deployment time, this means:\n\nBreeds can be referenced in blueprints before they exist \nYou do not need to know the contents of an SLA when you reference it.\nYou can reference a variable that someone else should fill in.\n\nRead more about referencing artifacts and environment variables.\n\n{{ note title=\"What next?\" }}\nRead about Vamp breeds\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":31},{"path":"/documentation/using vamp/blueprints","date":"2016-09-13T09:00:00+00:00","title":"Blueprints","content":"\nBlueprints are execution plans - they describe how your services should be hooked up and what their topology should look like at runtime. This means you reference your breeds (or define them inline) and add runtime configuration to them.\n\nBlueprints allow you to add the following extra properties:\n\nGateways: a stable port where the service can be reached.\nClusters and services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nScale: the CPU and memory and the amount of instance allocate to a service.\nConditions: how traffic should be directed based on HTTP and/or TCP properties.\nSLA and escalations: SLA definition that controls autoscaling.\n\nExample - key concepts of blueprints\n\n,name: my_blueprint                         Custom blueprint name\ngateways:\n  8080/http: my_frontend/port\nclusters:\n  my_frontend:                            # Custom cluster name.\n  \n    gateways:                             # Gateway for this cluster services.\n      routes:                             # Makes sense only with\n        somecoolbreed:                  # multiple services per cluster.\n          weight: 95%\n          condition: User-Agent = Chrome\n        someotherbreed:                 # Second service.\n          weight: 5%\n          \n    services:                             # List of services\n      breed:\n          ref: somecoolbreed\n        scale:                            # Scale for this service.\n          cpu: 2                          # Number of CPUs per instance.\n          memory: 2048MB                  # Memory per instance (MB/GB units).\n          instances: 2                    # Number of instances\n      breed: \n          ref: someotherbreed           # Another service in the same cluster.  \n        scale: large                      # Notice we used a reference to a \"scale\". \n                                          # More on this later.\n\nGateways\n\nA gateway is a \"stable\" endpoint (or port in simplified sense) that almost never changes. When creating the mapping, it uses the definition (my_frontend/port in this case) from the \"first\" service in the cluster definition you reference. This service can of course be changed, but the gateway port normally doesn't.\n\nPlease take care of setting the /tcp or /http (default) type for the port. Using /http allows Vamp to record more relevant metrics like response times and metrics.\n\nRead more about gateways.\n\n{{ note title=\"Note!\" }}\ngateways are optional. You can just deploy services and have a home grown method to connect them to some stable, exposable endpoint.\n{{ /note }}\n\n Clusters and services\n\nIn essence, blueprints define a collection of clusters.\nA cluster is a group of different services, which will appear as a single service and serve a single purpose.\n\nCommon use cases would be service A and B in an A/B testing scenario - usually just different\nversions of the same service (e.g. canary release or blue/green deployment).\n\nClusters are configured by defining an array of services. A cluster can be given an arbitrary name. Services are just lists or arrays of breeds.\n\n,mycoolcluster\n  services\n   breed: \n      ref: mycoolservice_A      # reference to an existing breed\n   breed:                       # shortened inline breed\n       name: mycoolservice_B\n       deployable: some_container\n       ...\n\nClusters and services are just organisational items. Vamp uses them to order, reference and control the actual containers and gateways and traffic.\n\n This all seems redundant, right? We have a reference chain of blueprints - gateways - clusters - services - breeds - deployable. However, you need this level of control and granularity in any serious environment where DRY principles are taken seriously and where \"one size fits all\" doesn't fly.\n\nDialects\n\nVamp allows you to use container driver specific tags inside blueprints. We call this a “dialect”.  Dialects effectively enable you to make full use of, for instance, the underlying features like mounting disks, settings commands and providing access to private Docker registries.\n\nWe currently support the following dialects:\n\ndocker:\nmarathon:\n\n Docker dialect\n\nThe following example show how you can mount a volume to a Docker container using the Docker dialect.\n\nExample blueprint - using the Docker dialect\n\n,name: busybox\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox-breed\n        deployable: busybox:latest\n      docker:\n        Volumes:\n          \"/tmp\": ~\n\nVamp will translate this into the proper API call. Inspecting the container after it's deployed should show something similar to this:\n\n...\n\"Volumes\": {\n      \"/tmp\": \"/mnt/sda1/var/lib/docker/volumes/1a3923fa6108cc3e19a7fe0eeaa2a6c0454688ca6165d1919bf647f5f370d4d5/_data\"\n  },\n...    \n\n Marathon dialect\n\nThis is an example with Marathon that pulls an image from private repo, mounts some volumes, sets some labels and gets run with an ad hoc command: all taken care of by Marathon.\n  \nWe can provide the marathon: tag either on the service level, or the cluster level. Any marathon: tag set on the service level will override the cluster level as it is more specific. However, in 9 out of 10 cases the cluster level makes the most sense. Later, you can also mix dialects so you can prep your blueprint for multiple environments and run times within one description.\n\nexample blueprint - using the Marathon dialect\n\nNotice the following:\n\nUnder the marathon: tag, we provide the command to run in the container by setting the cmd: tag.\nWe provide a url to some credentials file in the uri array. As described in the Marathon docs (mesosphere.github.io/marathon - using a private Docker repository) this enables Mesos\nto pull from a private registry, in this case registry.example.com where these credentials are set up.\nWe set some labels with some arbitrary metadata.\nWe mount the /tmp to in Read/Write mode.\n\n,name: busy-top:1.0\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox\n        deployable: registry.example.com/busybox:latest\n      marathon:\n       cmd: \"top\"\n       uris:\n         \"https://somehost/somepath/somefilewithdockercredentials\"\n       labels:\n         environment: \"staging\"\n         owner: \"buffy the vamp slayer\"\n       container:\n         volumes:\n           containerPath: \"/tmp/\"\n             hostPath: \"/tmp/\"\n             mode: \"RW\"\n\n Scale\n\nScale is the \"size\" of a deployed service. Usually that means the number of instances (servers) and allocated CPU and memory.\n\nScales can be defined inline in a blueprint or they can defined separately and given a unique name. The following example is a scale named \"small\". POST-ing this scale to the /scales REST API endpoint will store it under that name so it can be referenced from other blueprints.\n\nExample scale\n\n,name: small    Custom name.\n\ncpu: 2        # Number of CPUs per instance.\nmemory: 2gb   # Memory per instance, MB/GB units.\ninstances: 2  # Number of instances.\n\n{{ note title=\"What next?\" }}\nRead about Vamp deployments\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n","id":32},{"path":"/documentation/using vamp/breeds","date":"2016-09-13T09:00:00+00:00","title":"Breeds ","content":"Breeds are static descriptions of applications and services available for deployment. Each breed is described by the DSL in YAML notation or JSON, whatever you like. This description includes name, version, available parameters, dependencies etc.\nTo a certain degree, you could compare a breed to a Maven artifact or a Ruby Gem description.\n\nBreeds allow you to set the following properties:\n\nDeployable: the name of actual container or command that should be run.\nPorts: a map of ports your container exposes.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDependencies: a list of other breeds this breed depends on.\n\nDeployable\n\nDeployables are pointers to the actual artifacts that get deployed. Vamp supports Docker containers or can support any other artifacts supported by your container manager. \n\n Example breed - deploy a Docker container\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http   \n\nThis breed, with a unique name, describes a deployable and the port it works on. \n\nDocker deployables\n\nBy default, the deployable is a Docker container. \nWe could also make this explicit by setting type to docker. The following statements are equivalent:\n\n,deployable: company/myfrontendservice:0.1\n\n,deployable: \n  type: docker\n  definition: company/myfrontendservice:0.1\n\nThis shows the full (expanded) deployable with type and definition.\n\nDocker images are pulled by your container manager from any of the repositories configured. By default that would be the public Docker hub, but it could also be a private repo.\n\n Other deployables\n\nRunning \"other\" artifacts such as zips or jars heavily depends on the underlying container manager.\nWhen Vamp is set up to run with Marathon (mesosphere.github.io - Marathon), command (or cmd) deployable types can be used.\nIn that case cmd (Marathon REST API - post v2/apps) parameter will have value of deployable.\n\nExample breed - run a custom jar after it has been downloaded \nCombining this definition and the Vamp Marathon dialect  uris parameter allows the requested jar to be downloaded from a remote location (Marathon REST API - uris Array of Strings). \n\n,name: location\nclusters:\n  api:\n    services:\n      breed:\n        name: location\n        deployable: \n          type: cmd\n          definition: java -jar location.jar\n      marathon:\n        uris: [\"https://myrepolocation_jar\"]\n\n JavaScript deployables \n\nBreeds can have type application/javascript and definition should be a JavaScript script:\n\n,name: hello-world\ndeployable:\n  type: application/javascript\n  definition: |\n    console.log('Hello World Vamp!');\n\nIt is possible to create or update breeds with the API request POST|PUT /api/v1/breeds/{name}, Javascript script as body and header Content-Type: application/javascript.\n\nPorts\n\nThe ports property is an array of named ports together with their protocol. It describes on what ports the deployables is offering services to the outside world. Let's look at the following breed:\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http\n  admin: 8081/http\n  redis: 9023/tcp   \n\nPorts come in two flavors:\n\n/http HTTP ports are the default type if none is specified. They are always recommended when dealing with HTTP-based services. Vamp can record a lot of interesting metrics like response times, errors etc. Of course, using /tcp will work but you miss out on cool data.\n/tcp Use TCP ports for things like Redis, MySQL etc.\n\n{{ note title=\"Note!\" }}\n/http notation for ports is required for use of filters.\n{{ /note }}\n\nNotice we can give the ports sensible names. This specific deployable has web port for customer traffic, an admin port for admin access and a redis port for some caching probably. These names come in handy when we later compose different breeds in blueprints.\n\n{{ note title=\"What next?\" }}\nRead about Vamp blueprints\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n","id":33},{"path":"/documentation/using vamp/conditions","date":"2016-09-13T09:00:00+00:00","title":"Conditions","content":"\nConditions are used by gateways to filter incoming traffic for routing between services in a cluster.\nRead more about gateway usage. You can define conditions inline in a blueprint or store them separately under a unique name on the /conditions endpoint and just use that name to reference them from a blueprint. \n\nExample - simple inline condition\n\nThis would be used directly inside a blueprint.\n\n,condition_strength: 10%   Amount of traffic for this service in percents.\ncondition: User-Agent = IOS\n\nCreate a condition \n\nCreating conditions is quite easy. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv - 7.1 ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a condition.\n\nHowever, ACL's can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following:\n\n| description           | syntax                       | example                  |\n| ----------------------|:----------------------------:|:------------------------:|\n| match user agent      | user-agent == value          | user-agent == Firefox    |\n| mismatch user agent   | user-agent != value          | user-agent != Firefox    |\n| match host            | host == value                | host == localhost        |\n| mismatch host         | host != value                | host != localhost       |\n| has cookie            | has cookie value             | has cookie vamp          |\n| misses cookie         | misses cookie value          | misses cookie vamp       |\n| has header            | has header value             | has header ETag          |\n| misses header         | misses header value          | misses header ETag       |\n| match cookie value    | cookie name has value    | cookie vamp has 12345    |\n| mismatch cookie value | cookie name misses value | cookie vamp misses 12345 |\n| header has value      | header name has value   | header vamp has 12345    |\n| header misses value   | header name misses value | header vamp misses 12345 |\n\nAdditional syntax examples: github.com/magneticio/vamp - ConditionDefinitionParserSpec.scala.\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nIn order to specify plain HAProxy ACL, ACL needs to be between { }:\n\ncondition: \" hdr_sub(user-agent) Chrome \"\n\nHaving multiple conditions in a condition is perfectly possible. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header\n\"X-VAMP-MY-COOL-HEADER\". So any request matching both conditions would go to this service.\n\n,gateways:\n  weight: 100%\n  condition: \"User-Agent = Chrome AND Has Header X-VAMP-MY-COOL-HEADER\"\n\nUsing a tool like httpie (github.com/jkbrzt/httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-MY-COOL-HEADER:stuff\n\n Boolean expression in conditions\n\nVamp supports AND, OR, negation NOT and grouping ( ):\n\n,gateways:\n  weight: 100%\n  condition: (User-Agent = Chrome OR User-Agent = Firefox) AND has cookie vamp\n\nAdditional boolean expression examples: github.com/magneticio/vamp - BooleanParserSpec.scala.\n\n{{ note title=\"What next?\" }}\nRead about Vamp events\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n","id":34},{"path":"/documentation/using vamp/deployments","date":"2016-09-13T09:00:00+00:00","title":"Deployments","content":"\nA deployment is a \"running\" blueprint. Over time, new blueprints can be merged with existing deployments or parts of the running blueprint can be removed from it. Each deployment can be exported as a blueprint and \ncopy / pasted to another environment, or even to the same environment to function as a clone.\n\nCreate a deployment\n\nYou can create a deployment in the following ways:\n\nSend a POST request to the /deployments endpoint.\nUse the UI to deploy a blueprint using the \"deploy\" button on the \"blueprints\" tab.\nUse the CLI vamp deploy command  \n $ vamp deploy my_blueprint.\n\nThe name of the deployment will be automatically assigned as a UUID (e.g. 123e4567-e89b-12d3-a456-426655440000).\n\n Vamp deployment process\n\nOnce we have issued the deployment, Vamp will do the following:\n\nUpdate Vamps internal model.\nIssue and monitor deployment commands to the container platform.\nUpdate the ZooKeeper entry.\nStart collecting metrics.\nMonitor the container platform for changes.\n\nVamp will add runtime information to the deployment model, like start times, resolved ports etc.\n\nDeployment scenarios\n\nA common Vamp deployment scenario is to introduce a new version of the service to an existing cluster, this is what we call a merge. After testing/migration is done, the old or new version can be removed from the cluster, simply called a removal. Let's look at each in turn.\n\n Merge\n\nMerging of new services is performed as a deployment update. You can merge in many ways:\n\nSend a PUT request to the /deployments/{deployment_name} endpoint.\nUse the UI to update a deployment using the \"Edit deployment\" button. \nUse the CLI with a combination of the vamp merge and vamp deploy commands.\n\nIf a service already exists then only the gateways and scale will be updated. Otherwise a new service will be added. If a new cluster doesn't exist in the deployment, it will be added.\n\nLet's deploy a simple service:\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    # Specifying only a reference to the breed.\n    breed: monarch_1.0   \n\nAfter this point we may have another version ready for deployment and now instead of only one service, we have added another one:\n\n,name: monarch_1.1\n\nenvironment_variables:\n  # Some variable needed for our new recommendation engine,\n  # just as an example.\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    # Just a reference and this breed has one dependency:\n    # recommendation_1.0\n    breed: monarch_1.1\n\n  recommendation:\n    breed: recommendation_1.0\n \n\nNow our deployment (in simplified blueprint format) looks like this:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 100%\n      monarch_1.1:\n        weight: 0%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    \n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nNote that the route weight for monarch_1.1 is 0, i.e. no traffic is sent to it.\nLet's redirect some traffic to our new monarch_1.1 (e.g. 10%):\n\n,clusters:\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 90%\n      monarch_1.1:\n        weight: 10%\n\nNote that we can omit other fields like name, parameters and even other clusters (e.g. recommendation) if the change is not relevant to them. In this example we just wanted to update the weights.\n\nIn the last few examples we have shown the following:\n\nA fresh new deployment.\nA canary release with a cluster update and change of the topology (a new cluster was added).\nAn update of the gateways for a cluster - similar to a cluster scale update (instances, cpu, memory).\n\nRemoval\n\nRemoval is done using the REST API DELETE request together with the new blueprint as request body.\nIf a service exists it will be removed, otherwise the request is ignored. If a cluster has no more services left the cluster will be removed completely. Lastly, if a deployment has no more clusters it will be completely removed (destroyed).\n\nLet's use the example from the previous section. Notice the weight is evenly distributed (50/50). \n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 50%\n      monarch_1.1:\n        weight: 50%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100\n\nIf we are happy with the new monarch version 1.1, we can proceed with the removal of the old version.\nThis change is applied on the running deployment. We send the following YAML as the body of the DELETE request\nto the /deployments/deployment_UUID endpoint.\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    breed: monarch_1.0\n\nNote that this is the same original blueprint we started with. What we are doing here is basically \"subtracting\" one blueprint from the other, although \"the other\" is a running deployment.\nAfter this operation our deployment is:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.1\n    gateways:\n      monarch_1.1:\n        weight: 100%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nIn a nutshell: If we say that the first version was A and the second B, then we just did the migration from A to B without downtime:\nA** - A + B - A + B - A - **B\n\nWe could also remove the newer version (monarch_1.1 with/without recommendation cluster) in case that it didn't perform as we expected.\n\n{{ note title=\"What next?\" }}\nRead about Vamp environment variables\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":35},{"path":"/documentation/using vamp/environment-variables","date":"2016-09-13T09:00:00+00:00","title":"Environment variables ","content":"\nBreeds and blueprints can include lists of environment variables that will be injected into the container at runtime. You set environment variables with the environment_variables keyword or its shorter version env, e.g. both examples below are equivalent.\n\n,environment_variables:\n  PORT: 8080\n\n,env:\n  PORT: 8080\n\nDependencies\n\nBreeds can also have dependencies on other breeds. These dependencies should be stated explicitly, similar to how you would do in a Maven pom.xml, a Ruby Gemfile or similar package dependency systems, i.e:\n\n,dependencies:\n  cache: redis:1.1\n\nIn a lot of cases, dependencies coexist with interpolated environment variables or constants because exact values are not known untill deploy time.\n\n 'Hard' setting a variable\n\nYou want to \"hard set\" an environment variable, just like doing an export MYVAR=somevalue in a shell. This  variable could be some external dependency you have no direct control over: the endpoint of some service you use that is out of your control. It can also be some setting you want to tweak, like JVMHEAPSIZE or AWS_REGION.\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\n\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1'     \n\nIt is also possible to use wildcard * at the end of the name:\n\n,dependencies:\n  cache: redis:1.*\n\nThis will match any breed name that starts with redis:1.\n\nUsing place holders\n\nUse the ~ character to define a place holder for a variable that should be filled in at runtime (i.e. when this breed actually gets deployed), but for which you do not yet know the actual value. \n\n{{ tip title=\"Typical use case\" }}\nWhen different roles in a company work on the same project. Developers can create place holders for variables that operations should fill in: it helps with separating responsibilities.\n{{ /tip }}\n\n Example - ORACLE_PASSWORD designated as a place holder\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1' \n  ORACLE_PASSWORD: ~    \n\nResolving variables\n\nUse the $ character to reference other statements in a breed/blueprint. This allows you to dynamically resolve ports and hosts names that we don't know until a deployment is done. You can also resolve to hard coded and published constants from some other part of the blueprint or breed, typically a dependency.\n\n{{ note title=\"Note!\" }}\nThe $ value is escaped by $$. A more strict notation is ${some_reference}\n{{ /note }}\n\n Vamp host variable\n\nVamp provides just one magic* variable: the host. This resolves to the host or ip address of the referenced service. Strictly speaking, the host reference resolves to the gateway agent endpoint, but users do not need to concern themselves with this. Users can think of one-on-one connections where Vamp actually does server-side service discovery to decouple services.\n\nExample - resolving variables from a dependency\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host         resolves to a host at runtime\n      MYSQL_PORT: $backend.ports.port  # resolves to a port at runtime\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n\nExample - resolving variables from a dependency's environment variables\nWhat if the backend is configured through some environment variable, but the frontend also needs that information? For example, the encoding type for our database. We can just reference that environment variable using the exact same syntax.\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host       \n      MYSQL_PORT: $backend.ports.port \n      BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n    environment_variables:\n      ENCODING_TYPE: 'UTF8'    injected into the backend MySQL container\n\nYou can do everything with environment_variables but constants (see below) allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\nEnvironment variable scope\n\nEnvironment variables can live on different scopes and can be overridden by scopes higher in the scope hierarchy.\nA scope is an area of your breed or blueprint definition that limits the visibility of variables and references inside that scope.\n\nBreed scope: The scope used in all the above examples is the default scope. If you never define any environment_variables in any other place, this will be used.\n\nCluster scope: Will override the breed scope and is part of the blueprint artifact. Use this to override environment variables for all services that belong to a cluster.\n\nService scope: Will override breed scope and cluster scope, and is part of the blueprint artifact. Use this to override all environment variables for a specific service within a cluster.\n\n{{ note title=\"Note!\" }} \nEffective use of scope is completely dependent on your use case. The various scopes help to separate concerns when multiple people and/or teams work on Vamp artifacts and deployments and need to decouple their effort.\n{{ /note }}\n\n Examples of scope use\n\nRun two of the same services with different configurations\nOverride the JVMHEAPSIZE in production\nUse a place holder\nJust for fun - combine all scopes and references\n\nExample 1\nRun two of the same services with different configurations\n\nUse case: As a devOps-er you want to test one service configured in two different ways at the same time. Your service is configurable using environment variables. In this case we are testing a connection pool setting. \n\nImplementation: In the below blueprint we just use the breed level environment variables. The traffic is split into a 50/50 divide between both services.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0-a\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 30                \n      breed:\n          name: frontend_app:1.0-b               different breed name, same deployable\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 60                 # different pool size\n\n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nExample 2\nOverride the JVM_HEAP_SIZE in production\n\nUse case: As a developer, you created your service with a default heap size you use on your development laptop and maybe on a test environment. Once your service goes \"live\", an ops guy/gal should be able to override this setting.\n\nImplementation: In the below blueprint we override the variable JVMHEAPSIZE for the whole frontend cluster by specifically marking it with .dot-notation cluster.variable \n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                   cluster level variable \n  frontend.JVMHEAPSIZE: 2800          # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            JVMHEAPSIZE: 1200         # will be overridden by deployment level: 2800\n\nExample 3\nUse a place holder\n\nUse case: As a developer, you might not know some value your service needs at runtime, say the Google Anaytics ID your company uses. However, your Node.js frontend needs it! \n\nImplementation: In the below blueprint the ~ place holder is used to explicitly demand that a variable is set by a higher scope. When this variable is NOT provided, Vamp will report an error at deploy time.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.GOOGLEANALYTICSKEY: 'UA-53758816-1'  # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/node_express:1.0\n          ports:\n            port: 8080/http\n          environment_variables:           \n            GOOGLEANALYTICSKEY: ~               # If not provided at higher scope, \n                                                  # Vamp reports error.\n\nExample 4\nCombine all scopes and references\n\nAs a final example, let's combine some of the examples above and include referenced breeds. In this case, we have two breed artifacts already stored in Vamp and include them by using the ref keyword.\n\nIn the below blueprint:  \n\nwe override all breed scope JVMHEAPSIZE variables with cluster scope environment_variables\nto further tweak the JVMHEAPSIZE for the service frontendapp:1.0-b, we also add service scope environmentvariables for that service.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.JVMHEAPSIZE: 2400                    # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          ref: frontend_app:1.0-a\n      breed:\n          ref: frontend_app:1.0-b        \n        environment_variables:           \n          JVMHEAPSIZE: 1800               # overrides the breed level AND cluster level\n          \n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nConstants\n\nSometimes you just want configuration information to be available in a breed or blueprint. You don't need that information to be directly exposed as an environment variable. As a convenience, Vamp allows you to set constants.\nThese are values that cannot be changed during deploy time.\n\nYou can do everything with environment_variables but constants allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\n Exammple - using constants\n\n,clusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      environment_variables:\n        MYSQL_HOST: $backend.host       \n        MYSQL_PORT: $backend.ports.port \n        BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n        SCHEMA: $backend.constants.SCHEMA_NAME\n      dependencies:\n        backend: mysql:1.0\n  backend:\n    breed:\n      name: mysql:1.0\n      environment_variables:\n        ENCODING_TYPE: 'UTF8'\n      constants:\n        SCHEMA_NAME: 'customers'    # NOT injected into the backend MySQL container\n\n{{ note title=\"What next?\" }}\nRead about Vamp gateways\nCheck the API documentation\nTry Vamp\n{{ /note }}","id":36},{"path":"/documentation/using vamp/escalations","date":"2016-09-13T09:00:00+00:00","title":"Escalations","content":"\nAn escalation is a workflow triggered by an escalation event. Vamp checks for these escalation events using a continuous background process with a configurable interval time. If the events match the escalation handlers defined in the DSL, the action is executed.\n\nAgain: escalation events can be generated by third party systems and they will be handled in the same manner as events created by Vamp SLA workflows. \n\nEscalation handlers\n\nAny escalation that is triggered should be handled by an escalation handler\n\nVamp ships with the following set of escalation handlers - scaleinstances, scalecpu and scale_memory. These handlers can be composed into intricate escalation systems.\n\n scale_instances   \nScales up the number of running instances. It is applied only to the first service in the cluster (old or \"A\" version). You can set upper limits to how far you want to scale out or in, effectively guaranteeing a minimum set of running instances. This is very much like AWS auto-scaling.  \nExample - scale_instances\n,type: scale_instances\ntarget: monarch   Target cluster for the scale up/down.\n                 # If it's not specified, by default it's the \n                 # current cluster where SLA escalations are \n                 # specified.\nminimum: 1       # Minimum number of instances.\nmaximum: 3       # Maximum number of instances.\nscale_by: 1      # Increment/decrement to use on current \n                 # number of running instances.\nscale_cpu \nScales up the number of CPUs per instances. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_cpu\n,type: scale_cpu\ntarget: monarch  \nminimum: 1\nmaximum: 3 \nscale_by: 0.5\nscale_memory   \nScales up the memory per instance. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_memory  \n,type: scale_memory\ntarget: monarch  \nminimum: 512     # In MB.\nmaximum: 4096    # In MB.\nscale_by: 512    # In MB.\n\nComposing escalation handlers\n\nVamp has a set of predefined escalation handler types that deal with escalations. You can compose these handlers in the DSL to get the desired outcome of an escalation event. The escalation handlers toall and toone are supported:\n\n to_all  \nThis is a \"group\" escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to all escalation handlers from its list. No order or hierarchy.    \n\nExample - to_all  \n,to_all:\n  escalations:\n     Scale up/down.\n    scale_instances\n    # And notify for each event.\n    notify\nto_one  \nThis is a group escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to each escalation handler (from its list) until one can handle it. On an Escalate event, it will start at the head of the list. During a DeEscalate event is will start at the rear.  \nWhen is this useful? Well, Vamp could try to scale up the service and if that;s not possible anymore (e.g. reached the upper limit of allowed scale) then an email can be sent.  \n\n Example - simple use of to_one\n,to_one:\n  escalations:\n    # First try to escalate.\n    scale_instances\n    # If it's not possible, proceed with notifying.\n    notify\nExample - complex use of to_one  \n,name: monarch\n\ngateways:\n  80: monarch1/port\n\nenvironment_variables:\n  monarch2.password: secret\n\nclusters:\n\n  monarch1:\n    breed:\n      name: monarch1\n      deployable: vamp/monarch1\n      ports:\n        port: 80/http\n      environment_variables:\n        STORAGE_HOST: $storage.host\n        DB_PORT: $storage.ports.port\n        STORAGEPASS: $storage.environmentvariables.password\n      \n      dependencies:\n        storage: monarch2\n\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n    \n    sla:\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000\n        lower: 100\n      window:\n        interval: 600\n        cooldown: 600\n      escalations:\n        to_one:\n            escalations:\n              type: scale_instances\n                 First try to scale up storage service.\n                target: monarch2\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              type: scale_instances\n                # If we cannot scale up storage anymore, scale up this.\n                target: monarch1\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              \n  monarch2:\n    breed:\n      name: monarch2\n      deployable: vamp/monarch2\n      ports:\n        port: 3306\n      environment_variables:\n        STORAGE_PASS: password\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n\n{{ note title=\"What next?\" }}\nRead about Referencing artifacts in Vamp\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":37},{"path":"/documentation/using vamp/events","date":"2016-09-13T09:00:00+00:00","title":"Events","content":"\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\n\nAll events are stored and retrieved using the Event API that is part of Vamp.\n\nExample - JSON \"deployment create\" event\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:create\"\n  ],\n  \"value\": \"name: sava\",\n  \"timestamp\": \"2015-04-21T09:15:42Z\"\n}\n\n Basic event rules\n\nAll events stick to some basic rules:\n\nAll data in Vamp are events. \nValues can be any JSON object or it can be empty.\nTimestamps are in ISO8601/RFC3339.\nTimestamps are optional. If not provided, Vamp will insert the current time.\nTimestamps are inclusive for querying.\nEvents can be tagged with metadata. A simple tag is just single string.\nQuerying data by tag assumes \"AND\" behaviour when multiple tags are supplied, i.e. [\"one\", \"two\"] would only fetch records that are tagged with both.\nSupported event aggregations are: average, min, max and count.\n\nHow tags are organised\n\nIn all of Vamp's components we follow a REST (resource oriented) schema, for instance:\n/deployments/{deployment_name} \n/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}\nTagging is done using a very similar schema: \"{resourcegroup}\", \"{resourcegroup}:{name}\". Some examples:\n\n\"deployments\", \"deployments:{deployment_name}\"\n\"deployments\", \"deployments:{deploymentname}\", \"clusters\", \"clusters:{clustername}\", \"services\", \"services\",services:{service_name} \"\n\nThis schema allows querying per group and per specific name. Getting all events related to all deployments is done by using tag \"deployments\". Getting events for specific deployment \"deployments:{deployment_name}\".\n\n Query events using tags\n\nUsing the tags schema and timestamps, you can do some powerful queries. Either use an exact timestamp or use special range query operators, described on the elastic.co site (elastic.co - Range query).\n\n{{ note title=\"Note!\" }}\nthe default page size for a set of returned events is 30.\n{{ /note }}\n\nExample queries\n\nGet all events\nResponse time for a cluster\nCurrent sessions for a service\nll known events for a service\n \n Example 1\nGet all events\n\nThe below query gets ALL metrics events up till now, taking into regard the pagination.\n\n{{ note title=\"Note!\" }}\nGET request with body - similar to approach used by Elasticsearch.\n{{ /note }}\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"metrics\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 2 \nResponse time for a cluster\n\nThe below query gets the most recent response time events for the \"frontend\" cluster in the \"d9b42796-d8f6-431b-9230-9d316defaf6d\" deployment.\n\nNotice the \"gateways:UUID\", \"metrics:responseTime\" and \"gateways\" tags. This means \"give me the response time of this specific gateway at the gateway level\". The response will echo back the events in the time range with the original set of tags associated with the events. \n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\"metrics:rtime\",\"route\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n[\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:35.001Z\",\n        \"type\": \"gateway-metric\"\n    },\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:32.001Z\",\n        \"type\": \"gateway-metric\"\n    }\n]    \n\n Example 3\nCurrent sessions for a service\n\nAnother example is getting the current sessions for a specific service, in this case the monarch_front:0.2 service that is part of the 214615ec-d5e4-473e-a98e-8aa4998b16f4 deployment and lives in the frontend cluster.\n\nNotice we made the search more specific by specifying the \"services\" and then \"service:SERVICE NAME\" tag.\nAlso, we are using relative timestamps: anything later or equal (lte) than \"now\".\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics:scur\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 4\nAll known events for a service\n\nThis below query gives you all the events we have for a specific service, in this case the same service as in example 2. In this way you can get a quick \"health snapshot\" of service, server, cluster or deployment.\n\nNotice we made the search less specific by just providing the \"metrics\" tag and not telling the API which specific one we want.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n Server-sent events (SSE)\n\nEvents can be streamed back directly from Vamp.\n\nGET /api/v1/events/stream\n\nIn order to narrow down (filter) events, list of tags could be provided in the request body.\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\"]\n}\n\nGET method can be also used with tag parameter (may be more convenient):\n\nGET /api/v1/events/stream?tag=archiving&tag=breeds\n\nArchiving\n\nAll changes in artifacts (creation, update or deletion) triggered by REST API calls are archived. We store the type of event and the original representation of the artifact. It's a bit like a Git log. \n\nHere is an example event:\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:delete\"\n  ],\n  \"value\": \"\",\n  \"timestamp\": \"2015-04-21T09:17:31Z\",\n  \"type\": \"\"\n}\n\nSearching through the archive is 100% the same as searching for events. The same tagging scheme applies.\nThe following query gives back the last set of delete actions executed in the Vamp API, regardless of the artifact type.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"archiving\",\"archiving:delete\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n{{ note title=\"What next?\" }}\nRead about Vamp SLA (Service Level Agreement)\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":38},{"path":"/documentation/using vamp/gateways","date":"2016-09-13T09:00:00+00:00","title":"Gateways","content":"\nGateways are dynmic runtime entities in the Vamp eco-system. They represent load balancer rules to deployment, cluster and service instances. There are two types of gateways:\n\nInternal gateways are created automatically for each deployment cluster and updated using the gateway/deployment API\nExternal gateways are explicitly declared either in a deployment blueprint or using the gateway API\n\nExample - automatically created gateway \n\nThe below gateway is for deployment vamp, cluster sava and port port.  \nThe cluster contains two services sava:1.0.0 and sava:1.1.0, each with two running instances. \n,name: vamp/sava/port            name\nport: 40000/http               # port, either http or tcp, assigned by Vamp\nactive: true                   # is it running - not in case of non (yet) existing routes\nsticky: none\nroutes:                        # routes\n  vamp/sava/sava:1.0.0/port:\n    weight: 50%\n    instances:\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda3c376-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31463\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda2d915-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31292\n  vamp/sava/sava:1.1.0/port:\n    weight: 50%\n    instances:\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa3c9e4-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31634\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa37bc3-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31826\n\nGateway Usage\n\nThe gateway API allows for programmable routing. External gateways give an entry point to clusters (optionally specified in deployment blueprints), and allow for canary releasing and A/B testing across deployments.\n\nA gateway defines a set of rules for routing traffic between different services within the same cluster.\nVamp allows you to determine this in three ways:\n\nA condition will target specific traffic. Read more about conditions  \nfor example, IOS users\nThe condition_strength targets a percentage of traffic matching a condition  \nfor example, 10% of IOS users\nThe weight for each available route (%) defines the distribution of all remaining traffic (not matching or not targetted by a condition)  \nfor example, 100% of all traffic, except the targetted 10% of IOS users\n\n Routes, condition-strength and weights\n\nEach route can have a weight and one or more conditions (see boolean expression in conditions). Each condition has a condition-strength.\n\nRouting is calculated as followed:\n\nFind the first condition that matches the request. Read more about conditions  \nfor example, IOS users\nIf the route exists, send the request to it depending on the condition strength  \nfor example, 10% of IOS users are sent to the route\nIf, based on condition strength, the request should not follow that route, then send request to one from all routes based on their weight.  \nfor example, all non-IOS traffic and 90% of IOS users are routed according to route weight\n\n{{ note title=\"Note!\" }}\nVamp has to account for all traffic.  \nWhen defining weights, the total weight of all routes must always add up to 100%.\nThis means that in a straight three-way split one service must be given 34% as 33%+33%+33%=99%.  1% can be a lot of traffic in high volume environments.\n{{ /note }}\n\nExample - Route all Firefox and only Firefox users to route service_B:\n\nservice_A:\n  weight: 100%\nservice_B:\n  weight: 0%\n  condition_strength: 100%\n  condition: user-agent == Firefox\n\n Example - Route half of Firefox users to serviceB, other half to serviceA (80%) or service_B (20%):\nNon Firefox requests will be just sent to serviceA (80%) or serviceB (20%).\nservice_A:\n  weight: 80%\nservice_B:\n  weight: 20%\n  condition_strength: 50%\n  condition: user-agent == Firefox\n\nExample - A/B test two deployments using route weight\nBelow is a basic example, similar to putting both deployments (sava:1.0.0 and sava:1.1.0) in the same cluster.  \nIt is easy to imagine having an older legacy application and the new one and doing a full canary release (or A/B testing) in seamless way by using gateways like this.\n\nDeployment 1: PUT /api/v1/deployments/sava:1.0\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nDeployment 2: PUT /api/v1/deployments/sava:1.1\n\n,name: sava:1.1\ngateways:\n  9060/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nGateway (90% / 10%): POST /api/v1/gateways\n\n,name: sava\nport: 9070/http\nroutes:\n  sava:1.0/sava/port:\n    weight: 90%           condition can be used as well\n  sava:1.1/sava/port:\n    weight: 10%\n\nURL path rewrite\n\nVamp supports URL path rewrite. This can be a powerful solution in defining service APIs (e.g. RESTful) outside of application service.  Path rewrite is defined in the format path: NEW_PATH if CONDITION, where:\n\nNEW_PATH new path to be used; HAProxy variables are supported, e.g. %[path]\nCONDITION condition using HAProxy directives, e.g. matching path, method, headers etc.\n\n Example\nroutes:\n  web/port1:\n    rewrites:\n    path: a if b\n  web/port2:\n    weight: 100%\n\nVamp managed and external routes\n\nVamp managed routes are in the format:\n\ngateway - pointing to another gateway, e.g. it is possible to chain gateways\ndeployment/cluster - pointing to deployment cluster, i.e. services are not 'visible'\ndeployment/cluster/service - pointing to specific service within deployment cluster\n\nAll examples above cover only Vamp managed routes.\nIt is also possible to route traffic to specific IP or hostname and port.\nIn that case IP or hostname and port need to be specified between brackets, e.g. [hostname:port] (and double quotes due to Yaml syntax).\n\nname: mesos\nport: 8080/http\nsticky: route\n\nroutes:\n  \"[192.168.99.100:5050]\":\n    weight: 50%\n  \"[localhost:5050]\":\n    weight: 50%\n\n{{ note title=\"What next?\" }}\nRead about Vamp conditions\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":39},{"path":"/documentation/using vamp/references","date":"2016-09-13T09:00:00+00:00","title":"Referencing artifacts","content":"\nWith any artifact, Vamp allows you to either use an inline notation or reference the artifact by name. For references, you use the reference keyword or its shorter version ref. Think of it like either using actual values or pointers to a value. This has a big impact on how complex or simple you can make any blueprint, breed or deployment. It also impacts how much knowledge you need to have of all the different artifacts that are used in a typical deployment or blueprint.\n\nVamp assumes that referenced artifcats (the breed called my_breed in the example below) is available to load from its datastore at deploy time. This goes for all basic artifacts in Vamp: SLA's, gateways, conditions, escalations, etc.\n\nExample - reference notation\n\ninline notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          name: my_breed\n          deployable: registry.example.com/app:1.0\n        scale:\n          cpu: 2\n          memory: 1024MB\n          instances: 4\nreference notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          reference: my_breed\n        scale:\n          reference: medium  \n\n Working with references\n\nWhen you begin to work with Vamp, you will probably start with inline artifacts. You have everything in one place and can directly see what properties each artifact has. Later, you can start specialising and basically build a library of often used architectural components. \n\nExample use of references\n\nCreate a library of containers\nFix scales per environment\nReuse a complex condition\n\n Example 1 \nCreate a library of containers\n\nUse case: You have a Redis container you have tweaked and setup exactly the way you want it. You want to use that exact container in all your environments (dev, test, prod etc.). \n\nImplementation: Put all that info inside a breed and use either the Vamp UI or API to save it (below). Now you can just use the ref: redis:1.0 notation anywhere in a blueprint.\n\nPOST /api/v1/breeds\n\n,name: redis:1.0\ndeployable: redis\nports: 6379/tcp\n\nExample 2\nFix scales per environment\n\nUse case: You want to have a predetermined set of scales you can use per deployment per environment. For instance, a \"mediumproduction\" should be something else than a \"mediumtest\".\n\nImplementation: Put all that info inside a scale and use either the Vamp API to save it (below). Now you can use the ref: mediumtest or ref: mediumprod notation anywhere a scale type is required.\n\nPOST /api/v1/scales\n\n,name: medium_prod\ncpu: 2\nmemory: 4096MB\ninstances: 3\n\n,name: medium_test\ncpu: 0.5\nmemory: 1024MB\ninstances: 1\n\n Example 3\nReuse a complex condition\n\nUse case: You have created a complex condition to target a specific part of your traffic. In this case users with a cookie that have a specific session variable set in that cookie. You want to use that condition now and then to do some testing. \n\nImplementation: Put all that info inside a condition and use either the Vamp API to save it (below). Now you can use the  ref: conditionemptyshopping_cart anywhere that condition is required.\n\n,name: conditionemptyshopping_cart\ncondition: Cookie SHOPSESSION Contains shoppingbasketitems=0 \n\n{{ note title=\"What next?\" }}\nRead about Vamp workflows\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":40},{"path":"/documentation/using vamp/sla","date":"2016-09-13T09:00:00+00:00","title":"SLA (Service Level Agreement)","content":"\nSLA stands for \"Service Level Agreement\". Vamp uses it to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. In essence, an SLA and its associated escalation is a workflow that is checked and controlled by Vamp based on the runtime behaviour of a service. SLAs and escalations are defined with the VAMP DSL.\n\nThe SLA event system\n\nYou can define an SLA for each cluster in a blueprint. A common example would be to check if the average response time of the cluster (averaged across all services) is higher or lower than a certain threshold. Under the hood, an SLA workflow creates two distinct events. These are are sent from Vamp and stored to Elasticsearch.\n\nEscalate for a specific deployment and cluster  \ne.g. if the response time is higher than the upper threshold.\nDeEscalate for a specific deployment and cluster  \ne.g. if the response time is lower than the lower threshold.\n\nSLA monitoring is a continuous background process with a configurable interval time. On each run an SLA workflow is executed for each deployment & cluster that has an SLA defined. Within the same SLA definition it's possible to define a list of escalations. Escalations are triggered by escalation events (Escalate/DeEscalate).\n\nThis means escalation events can be generated by the third party systems by sending them to Elasticsearch. This would allow scaling up or down to be triggered by basically any system that can POST a piece of JSON.\n\nSLA's are in essence pieces of code inside Vamp that stick to this event model and can use, if they want, the metrics and event data streaming out of Elasticsearch to make decisions on how things are and should be running.\n\n SLA types\n\nVamp currently ships with the following SLA types:\n\nresponsetimesliding_window\n\nResponse time with sliding window \n\nThe responsetimesliding_window SLA triggers events based on response times. \n\n Example - SLA defined inline in a blueprint.\n\nNotice the SLA is defined per cluster and acts on the first service in the cluster.\n\nNotice how the SLA is defined separately from the escalations. This is key to how Vamp approaches SLA's and how modular and extendable the system is.\n\n,name: sava\n\ngateways:\n  80: sava/webport\n\nclusters:\n\n  sava:                        # the sava cluster\n    services:\n      breed:\n        name: monarch\n        deployable: vamp/monarch\n        ports:\n          webport: 80\n\n      scale:\n        cpu: 1\n        memory: 1024MB\n        instances: 2\n\n    sla:                        # SLA applies to the first service in the sava cluster (monarch)\n      # Type of SLA.\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000   # Upper threshold in milliseconds.\n        lower: 100    # Lower threshold in milliseconds.\n      window:\n        interval: 600 # Time period in seconds used for\n                      # average response time aggregation.\n        cooldown: 600 # Time period in seconds. During this \n                      # period no new escalation events will \n                      # be generated. New event may be expected \n                      # not before cooldown + interval time has \n                      # been reached after the last event. \n     \n      # List of escalations.\n      escalations:\n        type: scale_instances\n          minimum: 1\n          maximum: 3\n          scale_by: 1\n\n{{ note title=\"What next?\" }}\nRead about Vamp escalations\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":41},{"path":"/documentation/using vamp/sticky-sessions","date":"2016-09-13T09:00:00+00:00","title":"Sticky Sessions","content":"\nVamp supports route and instance level sticky sessions.\n\nRoute Level\n\nA common use case is when the end users have to have the same experience in A/B testing setup thus they should get the same service always (either A or B).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: route                            setting the route level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nInstance Level \n\nA common use case is when the end users need to be served by the same instance (e.g. stateful application).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: instance                          setting the instance level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nOther Notes\n\nResetting the sticky value can be done by: sticky: none or sticky: ~ (setting it to null).\n\nSticky sessions can be also used for gateways:\n\n,name: sava:1.0\ngateways:\n  9050/http:\n    sticky: service\n    routes:            let's say we have 2 clusters: sava1 (90%) and sava2 (10%)\n      sava1/port:   \n        weight: 90%\n      sava2/port:\n        weight: 10%\nclusters:\n  sava1: \n    ...\n  sava2: \n    ...\n\n{{ note title=\"What next?\" }}\nRead about using Vamp with virtual hosts\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":42},{"path":"/documentation/using vamp/virtual-hosts","date":"2016-09-13T09:00:00+00:00","title":"Virtual Hosts","content":"\nVamp can be configured to support virtual host via HAProxy:\n\nvamp.operation.gateway {\n    virtual-hosts = true\n    virtual-hosts-domain = \"vamp\"\n}\n\nExample - Virtual hosts\n \n PUT ${VAMP_URL}/api/v1/deployments/runner with body:\n\n,name: runner\n\ngateways:\n  9070: runner1/port\n  9080: runner2/port\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nNow you can request:\n\n$ curl --resolve 9070.runner.vamp:80:${VAMPGATEWAYAGENT_IP} http://9070.runner.vamp\n{\"id\":\"1.0.0\",\"runtime\":\"CF2136D9CA81282E\",\"port\":8081,\"path\":\"\"}\n\n$ curl --resolve 9080.runner.vamp:80:${VAMPGATEWAYAGENT_IP} http://9080.runner.vamp\n{\"id\":\"2.0.0\",\"runtime\":\"1E188B006FF44AA6\",\"port\":8081,\"path\":\"\"}\n{{ note title=\"Note!\" }}\nIf you are running Vamp in one of the quick setups, ${VAMPGATEWAYAGENTIP} should have value of ${DOCKERHOST_IP} - See the hello world quick setup instructions.\n{{ /note }}\n\nVamp creates a virtual host for each gateway - name of the gateway (/ replaced with .) appended to value from vamp.gateway-driver.virtual-hosts-domain.\nIn case of above example:\n\n9050.runner.vamp\n9060.runner.vamp\nport.runner1.runner.vamp\nport.runner2.runner.vamp\n\nUsing Gateway API it is possible to get virtual hosts for each gateway, e.g.\nGET ${VAMP_URL}/api/v1/gateways\n\n Custom virtual hosts\n\nAs you could see each gateways has virtual_hosts field.\nUsing that field it is also possible to set list of custom virtual hosts.\nLet's see that in the following example:\n\n,name: runner\n\ngateways:\n  9080:\n    virtual_hosts: [\n      \"run.vamp.run\"\n    ]\n    routes:\n      runner1/port:\n        weight: 50%\n      runner2/port:\n        weight: 50%\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nIf you deploy this blueprint as runner and check gateway 9080/runner:\n\nGET ${VAMP_URL}/api/v1/gateways/runner/9080\n{\n  \"name\": \"runner/9080\",\n  \"virtual_hosts\": [\n    \"9080.runner.vamp\",\n    \"run.vamp.run\"\n  ]\n  ...\n\n$ curl --resolve run.vamp.run:80:${VAMPGATEWAYAGENT_IP} http://run.vamp.run\n{\"id\":\"1.0.0\",\"runtime\":\"C0013E858F213AE0\",\"port\":8081,\"path\":\"\"}\n\n9080.runner.vamp is added if configuration parameter vamp.operation.gateway.virtual-hosts is set, otherwise just custom virtual hosts if any.\n\n{{ note title=\"What next?\" }}\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":43},{"path":"/documentation/using vamp/workflows","date":"2016-09-13T09:00:00+00:00","title":"Workflows ","content":"\nA \"workflow\" is an automated change of the running system and its deployments and gateways. \nChanging the number of running instances based on metrics (e.g. SLA) is an example of a workflow. \nA workflow can be seen as a recipe or solution, however it has a more generic meaning not just related to \"problematic\" situations.\n\nAnother example is a workflow that will decide automatically if a new version, when doing a canary release, should be accepted or not. \nFor instance, push the route up to 50% of traffic to the new version, compare metrics over some time (e.g. frequency of 5xx errors, response time), change to 100% and remove the old version. \nThis workflow could define the rate of the transitions (e.g. 5% - 10% - 25%, ...) as well.\n\nRationale\n\nWorkflows allow closing the feedback loop: deploy, measure, react.\nVamp workflows are based on running separate services (breeds) and in its simplest form scripting can be used - e.g. application/javascript breeds. \nScripting allows experimentation with different features and if the feature is common and generic enough, it could be supported later in Vamp DSL.\nSince workflows are running breeds in similar way as in deployments (blueprints), all other breed features are supported - ports, environment variables etc.\n\n Workflow API\n\nEach workflow is represented as an artifact and they follow basic CRUD operation patterns as any other artifact:\n  /api/v1/workflows\n\nEach workflow has to have:\n\n name\n breed - either reference or inline definition, similar to blueprints\n schedule \n scale - optional\n environment_variables (or env)- overrides breed environment variables\n arguments- Docker arguments, overrides default configuration arguments and breed arguments\n\nExample:\n\n,name: metrics\nbreed: metrics   # breed reference, inline definition can be also used\nschedule: daemon\nscale:           # inline scale, reference definition can be also used, e.g. scale: small\n  cpu: 1\n  memory: 128MB\n  instances: 2\nenvironment_variables:\n  interval: 5s\n\nSchedule\n\nFollowing schedule types are supported:\n\ndaemon\nevent with tags (set)\ntime - period, start (optional, by default starts now) and repeat (optional, by default runs forever) \n\nExamples:\n\n,schedule: daemon\n  \n time schedule\n\nschedule:\n  time:\n    period: P1Y2M3DT4H5M6S\n    start: now # or e.g. start: 2016-12-03T08:15:30Z\n    repeat: 10\n\nevent schedule\n\nschedule:\n  event:  event with following tags will trigger the workflow\n  deployments:sava\n  cluster:runner\n  \nor shorten notation in case of single event (still array can be used as above)\n\nschedule:\n  event: archive:bluprints\n\n      \nTime schedule period is in ISO8601 repeating interval notation.\n\nExample:\n\n,name    : metrics\nschedule: daemon\nbreed   :\n  name: metrics\n  deployable:\n    type: application/javascript\n    definition: |\n      'use strict';\n      \n      var _ = require('lodash');\n      var vamp = require('vamp-node-client');\n      \n      var api = new vamp.Api();\n      var metrics = new vamp.Metrics(api);\n      \n      var period = 5;  // seconds\n      var window = 30; // seconds\n      \n      var process = function() {\n        api.gateways(function (gateways) {\n          _.forEach(gateways, function(gateway) {\n            metrics.average({ \n              ft: gateway.lookup_name \n            }, 'Tt', window, function(total, rate, responseTime) {\n              api.event(['gateways:' + gateway.name, 'metrics:rate'], rate);\n              api.event(['gateways:' + gateway.name, 'metrics:responseTime'], responseTime);\n            });\n          });\n        });ß\n      };\n      \n      setInterval(process, period * 1000);\n\n{{ note title=\"Note!\" }}\nProbably it would be better to keep breed as a reference and create breed as shown below:\n{{ /note }}\n\nPUT Content-Type: application/javascript /api/v1/breeds/metrics\n'use strict';\n\nvar _ = require('lodash');\nvar vamp = require('vamp-node-client');\n\nvar api = new vamp.Api();\nvar metrics = new vamp.Metrics(api);\n\nvar period = 5;  // seconds\nvar window = 30; // seconds\n\nvar process = function() {\n  api.gateways(function (gateways) {\n      _.forEach(gateways, function(gateway) {\n          metrics.average({ ft: gateway.lookup_name }, 'Tt', window, function(total, rate, responseTime) {\n              api.event(['gateways:' + gateway.name, 'metrics:rate'], rate);\n              api.event(['gateways:' + gateway.name, 'metrics:responseTime'], responseTime);\n          });\n      });\n  });\n};\n\nsetInterval(process, period * 1000);\n\nJavaScript breeds will be executed by Vamp Workflow Agent (github.com/magneticio - Vamp workflow agent).  \n\nFor additional JavaScript API check out Vamp Node Client (github.com/magneticio - Vamp node client) project.\n\n{{ note title=\"What next?\" }}\nRead about Sticky sessions\nCheck the API documentation\nTry Vamp\n{{ /note }}","id":44},{"path":"/index.json","title":"","date":"","content":"---\ndate: 2016-08-02T01:13:07+02:00\ntype: json\nurl: index.json\n---","id":45},{"path":"/","date":"2016-09-13T09:00:00+00:00","title":"Canary releasing and autoscaling for microservice systems","type":"index","content":"try Vamp\nlearn more\n\n--------,\nPercentage and condition based routing   \nPrevent performance issues and service disruption with gradual rollouts and upgrades.\nIntegrated metrics- and event-driven workflows   \nAutomatically scale and optimise deployments or running systems.\nContainer-scheduler agnostic API   \nAvoid vendor lock-in with YAML based deployments.\n\n-------,\nVamp works with:  \n\nMesosphere  \nKubernetes  \nDC/OS  \nRancher  \nDocker\nAzure\n\n Cloud packets:  \n\nAWS\nGoogle\n\nCompanies using Vamp:  \n\nBBVA  \nExact  \nMijn domein  \nWehkamp  \nRabobank  \nZiggo\n\n----------,\n Vamp feature list\n\ngraphical UI and dashboard\nintegrated javascript-based workflow system\nmetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nautomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nopen source (Apache 2.0)\nevent API and server-side events (SSE) stream\nmulti-level metric aggregation\nport-based, virtual host names or external service (consul etc) based service discovery support\nlightweight design to run in high-available mission-critical architectures\nintegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing  \n\n----------,\nStay up to date\nsign up for mailing list\n","id":46},{"path":"/resources/community","date":"2016-09-13T09:00:00+00:00","title":"Join the Vamp community","content":"Vamp is an open source project, actively developed by Magnetic.io. We encourage anyone to pitch in with pull requests, bug reports etc. \n\nContribute to Vamp \nVamp is split into separate repos and projects. Check the source on Github for an overview of all key repos (github.com - magneticio).   \nFeel free to contribute with Github pull requests.\n\n Submit change or feature requests \nLet us know your change or feature requests.  \nSubmit an issue on github tagged \"feature proposal\". \n\nReport a bug \nif you find  bug, please report it!  \nSubmit an issue on github, including details of the environment you are running Vamp in.\n","id":47},{"path":"/resources/downloads/","date":"2016-10-19T09:00:00+00:00","title":"Downloads","content":"\nBinaries\nVamp\nVamp Gateway Agent (VGA)\nVamp CLI\n\n Homebrew\nVamp CLI for MacOS X\n\nDocker images\nVamp Gateway Agent (VGA) and HAProxy\nVamp workflow agent\n\n Build from source\nBuild Vamp\nBuild Vamp Gateway Agent (VGA)\n\n--------,\nBinaries\n\n Vamp\nDownload: bintray.com/magnetic-io - Vamp  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\nExample\nLet's assume that the Vamp binary is vamp.jar.\njava -Dlogback.configurationFile=logback.xml -Dconfig.file=application.conf -jar vamp.jar\n\nlogback.xml is the log configuration file (example logback.xml file)  \nVamp uses the Logback library. Additional information about using Logback and the log file configuration format can be found on the Logback project page (logback.qos.ch).\napplication.conf is the main Vamp configuration file (example application.conf file)  \nDefault values (github.com/magneticio - reference.conf) are loaded on start and application.conf may override any of them.\nProcessing configuration is based on the typesafe library. Additional information about syntax and usage can be found on the project page (github.com/typesafehub - config).\n\n Vamp Gateway Agent (VGA)\n\nDownload: bintray.com/magnetic-io - Vamp Gateway Agent\n\nDocumentation can be found on the project page (github.com/magneticio - Vamp Gateway Agent).\nVamp CLI\n\nDownload: bintray.com/magnetic-io - Vamp CLI  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\n Manual install - Windows and Linux\nInside the extracted Vamp CLI binary package (bintray.com/magnetic-io - Vamp CLI) is a bin directory. Add it to your PATH statement, open a Console/CMD window and type vamp.  \nAfter installation, set Vamp’s host location:\n\nVamp’s host location specified as a command line option ( --host )\n\nvamp list breeds --host=http://192.168.59.103:8080\n\nVamp’s host location specified via the environment variable VAMP_HOST\n\nexport VAMP_HOST=http://192.168.59.103:8080\n\nHomebrew\n Vamp CLI for MacOS X\nDownload: We have Homebrew support to install the Vamp CLI on MacOS X  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher  \n\nHomebrew install - MacOS X\n\nbrew tap magneticio/vamp\nbrew install vamp\n\nAfter installation, check if everything is properly installed with vamp version, then export the location of the Vamp host and check that the CLI can talk to Vamp:\nexport VAMP_HOST=http://localhost:8080\nvamp info\n\n Docker images\n\nVamp Gateway Agent (VGA) and HAProxy\nVamp Gateway Agent (VGA) Docker images with HAProxy can be pulled from the Docker hub (hub.docker.com - magneticio Vamp Gateway Agent).\n\n Vamp Workflow Agent\nA container for running small JavaScript-based workflows can be pulled from the Docker hub (hub.docker.com - magneticio Vamp workfow agent).\n Usually this will be pulled automatically.\n\nBuild from source\n\n Build Vamp\nRequirements: OpenJDK or Oracle Java version of 1.8.0_40 or higher, git (git-scm.com), sbt (scala-sbt.org), npm (npmjs.com) and Gulp (gulpjs.com)  \n{{ note }}\nIf you build from source (master branch) without a specific tag, you will build katana not the official release. Check the katana documentation for details of all changes since the last official release.\n{{ /note }}\n\nCheckout the source from the official repo (github.com/magneticio - Vamp):   \n  {{ note title=\"Note!\" }}\n  master branch contains the latest released version (e.g. 0.9.1). Versions are tagged.\n  vamp-ui is a separate project added as a git submodule to Vamp (ui subdirectory) it is, therefore, necessary to also checkout the submodule  \n  {{ /note }}\n  git clone --recursive git@github.com:magneticio/vamp.git  \n  or specific branch: git clone --recursive --branch 0.9.1 git@github.com:magneticio/vamp.git\n\nRun ./build-ui.sh && sbt test assembly\nAfter the build ./bootstrap/target/scala-2.11 directory will contain the binary with name matching vamp-assembly-*.jar\n\nCheck this example: github.com/magneticio - Vamp docker quick start make.sh.\n\nRunning Vamp\n\nLet’s assume that Vamp binary is vamp.jar. OpenJDK or Oracle Java version of 1.8.0_40 or higher needs to be installed. Check the version with java -version\n\nExample:\n\n`java -Dlogback.configurationFile=logback.xml -Dconfig.file=application.conf -jar vamp.jar\n\nlogback.xml` is log configuration. Vamp uses the Logback library and additional information about using the Logback and log file configuration format can be found on the Logback project page. An example file can be found here.\n\napplication.conf is the main Vamp configuration file. Default values are loaded on start and application.conf may override any of them. Processing configuration is based on this library. Additional information about syntax and usage can be found on the library project page. An example of configuration can be found here.\n\n Build Vamp Gateway Agent (VGA)\n\nRequirements: Go (golang.org) and git (git-scm.com)\n{{ note }}\nIf you build from source (master branch) without a specific tag, you will build katana not the official release. Check the katana documentation for details of all changes since the last official release.\n{{ /note }}\n\nCheckout the source from the official repo (github.com/magneticio - Vamp gateway agent). Current master branch is backward compatible with the latest 0.9.1 Vamp build.\nSet Go variables depending on target environment\nRun:\n\ngo get github.com/tools/godep\ngodep restore\ngo install\nCGO_ENABLED=0 go build -v -a -installsuffix cgo\n\nCheck this example: github.com/magneticio - clique-base make.sh. More details can found on the project page: github.com/magneticio - Vamp gateway agent.\n","id":48},{"path":"/support","date":"2016-09-13T09:00:00+00:00","title":"support   ","content":"Vamp is actively developed by Magnetic.io. Vamp Community Edition is open source and Apache 2.0 licensed.\n\nFor the Vamp Enterprise Edition please contact us to discuss your requirements, pricing and features.\n\nCommunity support\nIf you have a question about Vamp, please check the Vamp documentation first  - we're always adding new resources, tutorials and examples.\n\n Bug reports\nIf you found a bug, please report it! Create an issue on GitHub and provide as much info as you can, specifically the version of Vamp you are running and the container driver you are using.\n\nGitter\nYou can post questions directly to us on our public Gitter channel  \n\n Twitter\nYou can also follow us on Twitter: @vamp_io\n\nProfessional support\nFor extended support, pricing information on the Vamp Enterprise Edition (EE), professional services or consultancy you can contact us at info@magnetic.io or call +31(0)88 555 33 99\n\n{{ note title=\"What next?\" }}\nTry Vamp\nLearn how Vamp works\nGet your teeth into the Vamp documentation\n{{ /note }}\n","id":49},{"path":"/why use vamp/enterprise-edition","date":"2016-09-13T09:00:00+00:00","title":"Vamp Enterprise Edition","content":"\nFor features, pricing and availability of our commercial Vamp Enterprise Edition (EE), please contact info@magnetic.io or call +31(0)88 555 33 99\n","id":50},{"path":"/why use vamp/feature-list","date":"2016-09-13T09:00:00+00:00","title":"Feature list","content":"\nVamp 0.9.1 includes:\n\nContainer-scheduler agnostic API\nPercentage and condition based programmable routing\nYAML based configuration blueprints with support for dependencies, clusters and environment variables\nGraphical UI and dashboard\nIntegrated javascript-based workflow system\nMetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nAutomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nOpen source (Apache 2.0)\nEvent API and server-side events (SSE) stream\nMulti-level metric aggregation\nPort-based, virtual host names or external service (consul etc) based service discovery support\nLightweight design to run in high-available mission-critical architectures\nIntegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing\n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}\n","id":51},{"path":"/why use vamp/get-started","date":"2016-09-13T09:00:00+00:00","title":"Get started","content":"\nTry Vamp\nTo make the most of your 'my first Vamp' experience, we suggest you start by installing our single, all-in-one Vamp Docker Hello World package. This will set up everything you need to play around with Vamp on a local or remote dev machine - the container package includes Mesos/Marathon and Elastic Stack (ELK), as well as all the necessary Vamp components. We have some nice tutorials to get a feel for the supernatural powers of Vamp.\n\n Install a production-grade Vamp setup\nOf course our Hello World package is no production-grade setup. We suggest your next step should be to understand the Vamp architecture and then find the Vamp version for your favorite container scheduler. We support most common container schedulers, so you should be able to find one to your liking in our installation docs. If you're still not sure which container scheduler to work with, our 'what to choose' guide can help you make an informed decision.\n\nFine tune and integrate\nAfter you've successfully installed a production-grade Vamp on your preferred container-cluster manager/scheduler (if you need help here, find us in our public Gitter channel), it's time to either dive into the ways you can use Vamp or investigate how you can configure and fine-tune Vamp to match your specific requirements. It might also be interesting to integrate Vamp into your CI pipeline to create a CD pipeline with Vamp's canary-releasing features. You can check out our CLI and REST API documentation for integrations.\n\n Get your teeth into the fun stuff!\nAt this point you've become a real Vamp guru. The next step could be to start playing around with our Vamp Runner tool to investigate typical recipes, such as automated canary-releasing, auto-scaling and more. You can use the JavaScript-based workflows in the recipes as a reference to create your own recipes and workflows. Once you've created some cool workflows and recipes we would of course like to hear from you!\n\n{{ note title=\"What next?\" }}\nTry the Vamp Docker Hello World package and tutorials\nRead about the Vamp architecture\nCheck the installation docs\n{{ /note }}","id":52},{"path":"/why use vamp/","date":"2016-09-13T09:00:00+00:00","title":"Why use Vamp?","content":"\nWe recognise the pain and risk involved with delivering microservice applications.  We've been there too - facing downtime and unexpected issues while transitioning from one release to the next. \nIn microservice architectures, these concerns can quickly multiply. It's all too easy to get stuck dealing with the added complexities and miss out on the potential benefits. \n\nWhat is Vamp?\n\nVamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints and a runtime/execution engine to deploy these blueprints (similar to AWS Cloudformation). Planned deployments and running services can be managed from your choice of Vamp interface - graphical UI, command line interface or RESTful API. \n\nVamp takes care of route updates, metrics collection and service discovery, so you can easily orchestrate complex deployment patterns, such as architecture level A/B testing and canary releases.\nAfter deployment, Vamp workflows monitor running applications and can act automatically based on defined SLAs.  \n\n Vamp facts\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n{{ note title=\"What next?\" }}\nTry Vamp\nRead the full Vamp feature list\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}\n\n","id":53},{"path":"/why use vamp/see-vamp-in-action","date":"2016-09-13T09:00:00+00:00","title":"See Vamp in action","content":"\nWatch this video of running Vamp on DC/OS an doing a canary release.\n\nThe Vamp UI and DC/OS versions in this video are a little older than currently available. We will upload a new recording shortly.\n\n{{ youtube tlzfD1Uo9pk }}\n","id":54},{"path":"/why use vamp/use cases/create-responsive-website","date":"2016-09-13T09:00:00+00:00","title":"Canary test and release a responsive frontend","content":"\n“We need to upgrade our website frontend to make it responsive”\n  \nDeveloping a responsive web frontend is often a major undertaking, requiring a large investment of hours and extensive testing. Until you go live, it's difficult to predict how the upgrade will be received by users - will it actually improve important metrics, will it work on all browser, devices and resolutions, etc.?   \n\nBut why develop this new responsive frontend in one go, having to go for the dreaded and risky big-bang release? Using Vamp you can apply a canary release to introduce the new frontend to a selected cohort of users, browsers and/or devices. This would require a minimal investment of development and delivering real usage data:\n\nStart small: Build the new frontend for only one specific browser/resolution first to measure effectiveness. Vamp can deploy the new responsive frontend and route a percentage of supported users with this specific browser and screen-resolution there. All other users will continue to see the old version of your website.\nOptimise: With the new responsive frontend in the hands of real users with this specific browser/resolution, you can measure actual data and optimise accordingly without negatively affecting the majority of your users.\nScale up: Once you are satisified with the performance of the new frontend, you can use Vamp to scale up the release, developing and canary releasing one browser/resolution at a time. Of course other cohort combinations are also possible, Vamp is open and supports all HAProxy ACL rules.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to resolve client-side incompatibilities after an upgrade\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":55},{"path":"/why use vamp/use cases/","date":"2016-09-13T09:00:00+00:00","title":"use cases","content":"\nThe integrated deployment, routing and workflow features of Vamp support a broad range of scenarios and industry verticals. We specifically see powerful use cases in the areas of testing in production, migrating to microservices, and realtime system optimisation. In this section we describe specific scenarios and how Vamp can effectively solve these.\n\nTesting in production \nUse canary testing and releasing to introduce a responsive web frontend. Read more ...\nResolve client-side incompatibilities after an upgrade. Read more ...\nA/B test architectural changes in production. Read more ...\n\n Migrating to microservices\nMove from VM based monoliths to modern microservices. Read more ...\n\nRealtime system optimisation\n\nWhat would happen if...... Simulate and test auto scaling behaviour. Read more ...\nSelf-healing and self-optimising. Read more ...\n\nWe're always interested to hear specific use-cases. If you have one to share, send us an email at info@magnetic.io","id":56},{"path":"/why use vamp/use cases/modernise-architecture","date":"2016-09-13T09:00:00+00:00","title":"Test and modernise architecture","content":"\n\"We want to switch to a NoSQL database for our microservices, but don't know which solution will run best for our purposes\"\n\nWith multiple NoSQL database options available, it's hard to know which is the best fit for your specific circumstances. You can try things out in a test lab, but the real test comes when you go live with production load.\n\nWhy guess? Using Vamp you could A/B test different versions of your services with different NoSQL backends, in production, and then use real data to make an informed and data-driven decision.   \n\nDeploy two versions: Vamp can deploy multiple versions of your architecture, each with a different database solution (or other configuration settings), then distribute incoming traffic across each.\nStress test: Use the metrics reported by Vamp to measure which option performs best in production.\nKeep the best performing option: Once you have made your decision, Vamp can route all traffic to your chosen architecture. Services from the alternative options will be drained to ensure customer experience is not impacted by the test.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to simulate and test scaling behaviour\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":57},{"path":"/why use vamp/use cases/refactor-monolithic-to-microsystems","date":"2016-09-13T09:00:00+00:00","title":"Move from monoliths and VM's to microservices","content":"\n“We want to move to microservices, but we can’t upgrade all components at once and want to do a gradual migration”\n  \nRefactoring a monolithic application to a microservice architecture is a major project. A big bang style re-write to upgrade all components at once is a risky approach and requires a large investment in development, testing and refactoring.\n\nWhy not work incrementally? Using Vamp's routing you could introduce new services for specific application tiers like the frontend or business logic layers, and move traffic with specific conditions to these new services. These services in turn can connect to your legacy systems again, using Vamp's proxying. A typical example would be introducing an angular based frontend or node.js based API microservice. You can send 2% of your incoming traffic to this new microservice frontend, which in turn connects with the legacy backend system. This way you can test your new microservices in a small and controlled way and avoid a big bang release. You can introduce new services one by one, test them in production and increase traffic until you migrated your entire application from a monolith to microservices.\n\nStart small: You can build e.g. one new frontend component. Vamp will deploy this to run alongside the legacy monolithic system.\nActivate smart routing: Vamp can route traffic behind the scenes, so a small percentage of visitors is sent to the new frontend service, while the new frontend is routed by Vamp to the legacy backend. You can continue transferring components from the legacy monolithic system to new microservices and Vamp can adapt the routing as you go.\nRemove legacy components: Once all services have been transferred from the legacy monolith, you can start removing components from the legacy system.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to test and modernise architecture\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":58},{"path":"/why use vamp/use cases/resolve-incompatibilities-after-upgrade","date":"2016-09-13T09:00:00+00:00","title":"Resolve client-side incompatibilities after an upgrade","content":"\n“We upgraded our website self-management portal, but our biggest client is running an unsupported old browser version”\n  \nLeaving an important client unable to access your services after a major upgrade is a big and potentially costly problem. The traditional response would be to rollback the upgrade asap - if that's even possible.  \n\nWhy rollback? Using Vamp's smart conditional routing you could send specific clients or browser-versions to an older version of your portal while others can enjoy the benefits of your new upgraded portal. Because Vamp supports SLA based autoscaling for (Docker) containers, you can deploy the old version on the same infrastructure as the new version is running on. This also avoids having to provision costly over dimensioned DTAP environments for only a small user-base, leveraging your existing infrastructure efficiently.\n\nRe-deploy: Vamp can (re)deploy a (containerised) compatible version of your portal to run side-by-side with the upgraded version.\nActivate smart routing: Vamp can route all users with e.g. a specific IP, browser or location to a compatible version of the portal. Other clients will continue to see the new upgraded portal.\nResolve the incompatibility: Once the client upgrades to a compatible browser Vamp can automatically route them to the new portal version.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to move from monoliths and VMs to microservices\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":59},{"path":"/why use vamp/use cases/self-healing-and-self-optimising","date":"2016-09-13T09:00:00+00:00","title":"Self-healing and self-optimising","content":"\n\"Our website traffic can be unpredictable, it's hard to plan and dimension the exact resources we're going to need to run within SLA's\"\n\nWhy overdimension your whole system? Using Vamp you can auto-scale individual services based on clearly defined SLAs (Service Level Agreements). It's also easy to create advanced workflows for up and down scaling, based on your application or business specific requirements. Vamp can also make sure that unhealthy and failing services are corrected based on clearly defined metrics and treshholds.\n\nSet SLAs: You can define SLA metrics, tresholds and escalation workflows. You can do this in Vamp YAML blueprints, modify our packaged workflows, or create your own workflow scripts for advanced use-cases.\nOptimise: Vamp workflows can automatically optimise your running system based on metrics that are relevant to your application or services.\nSleep easy: Vamp will track troughs and spikes in activity and automatically scale services up and down to match your SLAs. All scaling events will be logged. Unhealthy services can be healed by Vamp.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":60},{"path":"/why use vamp/use cases/service-discovery","date":"2016-09-13T09:00:00+00:00","title":"Service discovery","content":"\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other service-discovery daemon or agent. In addition to service discovery, Vamp also functions as a service registry. We recognise the following benefits of this pattern:\n\nNo code injection needed\nNo extra libraries or agents needed\nPlatform/language agnostic: it’s just HTTP\nEasy integration using ENV variables\n\nVamp doesn't need point-to-point wiring. Vamp uses environment variables that resolve to service endpoints Vamp automatically sets up and exposes. Even though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc. Vamp can also integrate with common service discovery solutions like Consul and read from these to setup the required routing automatically.\n\nSmartStack\nVamp can automatically deploy a VampGatewayAgent(VGA)+HAProxy on every node of your container cluster. This creates a so-called Layer 7 intra service network mesh which enables you to create a \"SmartStack\", an automated service discovery and registration framework originally coined and developed by AirBnB. More on the history and advantages of the SmartStack approach can be read here (nerds.airbnb.com - SmartStack: Service Discovery in the Cloud).\n\n{{ note title=\"What next?\" }}\nRead more about Vamp service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}\n","id":61},{"path":"/why use vamp/use cases/simulate-and-test-scaling-behaviour","date":"2016-09-13T09:00:00+00:00","title":"Simulate and test scaling behaviour","content":"\n\"How would our system react if... the number of users increased x10 ... the response time of a service increased with 20 seconds ... an entire tier of our application would be killed ...\"  \n\nYour company might dream of overnight success, but what if it actually happened? Stress tests rarely cater to extreme real world circumstances and usage patterns, and are often done on systems that are not identical to production environments. It's not uncommon the bottleneck sits in the system generating the load itself, so it's difficult to predict how your microservices would actually scale or to know if your planned responses will really help.\n\nWhy not find out for sure? Using Vamp you can test your services and applications against difficult to predict or simulate situations, mocking all kinds of metrics, and then validate and optimise the workflows that handle the responses, like for example auto up and down scaling. With the same workflows as would be running in production, on the same infrastructure, with the same settings.\n\nMock required load: Vamp can simulate (mock) high-stress situations for any kind of metric your system needs to respond to, without actually having to generate real traffic.\nOptimise: You can optimise your resource allocation and autoscaling configurations based on real validated behaviour under stress.\nIterate until you're certain: Vamp can repeat the tests until you're confident with the outcome. Then you can use the same scaling and optimising workflows in production.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for self healing and self optimising\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":62},{"path":"/why use vamp/vamp-compared-to/frameworks-and-tools","date":"2016-09-13T09:00:00+00:00","title":"Frameworks and tools","content":"\nVamp compared to CI/CD tools\nSpinnaker, Jenkins, Wercker, Travis, Bamboo    \nVamp closes the loop between the development and operations elements of a CI/CD pipeline by enabling the controlled introduction of a deployable into production (canary-test and canary-release) and feeding back runtime technical and business metrics to power automated optimisation workflows (such as autoscalers). Vamp integrates with CI/CD tools like Travis, Jenkins or Wercker to canary-release and scale the built deployables they provide. The initial deployment setup is defined in a YAML blueprint (e.g. deployable details, required resources, routing filters) and is typically provided by the CI tool as a template to the Vamp API. Vamp will then run, canary-release, monitor and scale the deployment based on the filters and conditions specified in the blueprint.\n\n Vamp compared to feature toggle frameworks\nLaunchDarkly, Togglz, Petri  \nFeature toggle frameworks use code level feature toggles to conditionally test new functionality in an application or service. Tools such as LaunchDarkly and Togglz work with these toggles to enable, for example, A/B testing and canary functionality. While there are many cases for using feature toggles, there are also times when feature toggling isn't the smartest choice.\nVamp allows controlled testing of new features without the need to adjust your code. To achieve this, Vamp controls traffic routing towards and between your applications and services based on blueprint descriptions of individual (micro)services and their dependencies. This makes sense on an application code level, offering increased security and reduced technical debt compared to maintaining toggles in your code, and provides a mature alternative for cases when feature toggles are not the appropriate choice.\n\nVamp compared to configuration management and provisioning tools\nPuppet, Ansible, Chef, Terraform    \nThe responsbilities of configuration management and infrastructure provisioning tools are often stretched to cover container deployment features. These tools were not intended for handling container deployments or for the dynamic management and routing traffic over these containers. Vamp has been designed and developed from the ground up specifically to fit these use cases.  \n\n Vamp compared to custom built solutions\nBuilding and maintaining a scalable and robust enterprise-grade system for canary-testing and releasing is not trivial. Vamp delivers programmable routing and automatic load balancing, deployment orchestration and workflows, as well as a powerful event system, REST API, graphical UI, integration testing tools and a CLI.  \n\nVamp compared to A/B and MVT testing tools\nOptimizely, VisualWebsiteOptimizer, Google Analytics, Planout  \nVamp enables canary testing versions of applications, effectively providing A/B and MVT testing of applications and services by deploying two or more versions of an application or service and dividing incoming traffic between the running versions. Vamp doesn't have a built-in analytics engine though, so the analysing of the relevant metrics needs to be done with a specific Vamp workflow or an external analytics engine. Results can be fed back to Vamp to automatically update routing rules and deployments to push a winning version to a full production release. Because of the flexible programmable routing and use of environment variables, Vamp can be used to canary test almost everything, from content and business logic to configuration settings and architectural changes.  \n\n Vamp compared to DevOps tools\nDeis, Flynn, Dokku  \nVamp has no ambition to provide a Heroku-like environment for containers. Vamp integrates programmable routing and load balancing, container deployments and orchestration to enable canary testing and canary releasing features. Vamp also adds metrics-driven workflows for auto-scaling and other optimisations. Vamp sees business as a first class citizen in DevOps teams, providing a graphical UI and tools for non-technical roles.   \n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases -  some Vamp solutions to practical problems\nFind out how Vamp works\n{{ /note }}\n\n","id":63},{"path":"/why use vamp/vamp-compared-to/paas-and-container-systems","date":"2016-09-13T09:00:00+00:00","title":"PaaS and container systems","content":"\nVamp compared to container schedulers and container clouds (CPaaS)\nDocker Swarm, DC/OS, Mesos/Marathon, Kubernetes, Nomad, Rancher, AWS ECS, Azure CS, Mantl, Apollo  \nContainer cluster managers and schedulers like Marathon, DC/OS, Kubernetes, Nomad or Docker Swarm provide great features to run containers in clustered setups. What they don't provide are features to manage the lifecycle of a microservices or container based system. How to do continuous delivery, how to gradually introduce and upgrade versions in a controlled and risk-free way, how to aggregate metrics and how to use these metrics to optimise and scale your running system. Vamp adds these features on top of well-used container schedulers by dynamically managing routing and load balancing, deployment automation and metric driven workflows. Vamp also adds handy features like dependencies, ordering of deployments and resource management.\n\n Vamp compared to PaaS systems\nCloud foundry, OpenStack, IBM Bluemix, Openshift  \nVamp adds an experimentation layer to PaaS infrastructures by providing canary-releasing features that integrate with common PaaS proxies like HAProxy. For continuous delivery and auto-scaling features, Vamp integrates with common container-schedulers included in PaaS systems, like Kubernetes in Openshift V3.   \n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to common frameworks and tools\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n","id":64},{"path":"/why use vamp/vamp-compared-to/proxies-and-load-balancers","date":"2016-09-13T09:00:00+00:00","title":"Proxies and load balancers","content":"\nVamp compared to software based proxies and load balancers\nHAProxy, NGINX, linkerd, Traefik   \nVamp adds programmable routing (percentage and condition based) and load balancing to the battle-tested HAProxy proxy, as well as a REST API, graphical UI and CLI.  This means you can use Vamp together with all common container-schedulers to provide continuous delivery and auto-scaling features using automatic load balancing and clustering of scaled out container instances. By default Vamp is packaged with HAProxy, but you could also integrate the Vamp magic with other programmable proxies such as NGINX, linkerd or Traefik.\n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to PaaS and container systems\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n","id":65}]